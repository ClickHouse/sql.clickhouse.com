{
  "queries": [
    {
      "id": "12EBW5JKMBVOKUUANQPR1M",
      "name": "Disk space usage",
      "group": "amazon_reviews",
      "comment": "Displays storage metrics including compression ratios, row counts and partition data for active table segments",
      "query": "SELECT\n    disk_name,\n    formatReadableSize(sum(data_compressed_bytes) AS size) AS compressed,\n    formatReadableSize(sum(data_uncompressed_bytes) AS usize) AS uncompressed,\n    round(usize / size, 2) AS compr_rate,\n    sum(rows) AS rows,\n    count() AS part_count\nFROM system.parts\nWHERE (active = 1) AND (table = 'amazon_reviews')\nGROUP BY disk_name\nORDER BY size DESC;",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "disk-space-usage"
    },
    {
      "id": "BWOLVGMCMNCC3JDWF8EQGB",
      "name": "Top 10 most-helpful reviews",
      "group": "amazon_reviews",
      "comment": "Displays product titles and headlines for the 10 most helpful customer feedback entries",
      "query": "SELECT\n    product_title,\n    review_headline\nFROM amazon.amazon_reviews\nORDER BY helpful_votes DESC\nLIMIT 10;",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "top-10-most-helpful-reviews"
    },
    {
      "id": "2DSBPSYXKLHRWNWMDPSUJN",
      "name": "Top 10 products with most reviews",
      "group": "amazon_reviews",
      "comment": "Lists titles and feedback counts for the 10 most discussed items in the catalog",
      "query": "SELECT\n    any(product_title),\n    count()\nFROM amazon.amazon_reviews\nGROUP BY product_id\nORDER BY 2 DESC\nLIMIT 10;",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "top-10-products-with-most-reviews"
    },
    {
      "id": "OJJIEGD4NZK6KWH8FHYD64",
      "name": "Avg review ratings per month per product",
      "group": "amazon_reviews",
      "comment": "Shows monthly average star ratings for each item, sorted chronologically with newest first",
      "query": "SELECT\n    toStartOfMonth(review_date) AS month,\n    any(product_title),\n    avg(star_rating) AS avg_stars\nFROM amazon.amazon_reviews\nGROUP BY\n    month,\n    product_id\nORDER BY\n    month DESC,\n    product_id ASC\nLIMIT 20;",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "avg-review-ratings-per-month-per-product"
    },
    {
      "id": "C8VZQAGWJKHVQFFTRB2R7B",
      "name": "Total number of votes per product category",
      "group": "amazon_reviews",
      "comment": "Aggregates total user voting activity across different merchandise categories",
      "query": "SELECT\n    sum(total_votes),\n    product_category\nFROM amazon.amazon_reviews\nGROUP BY product_category\nORDER BY 1 DESC\n",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "total-number-of-votes-per-product-category"
    },
    {
      "id": "AT61M2PNSTBFNVKESBTJWE",
      "name": "Find products with 'awful' in reviews",
      "group": "amazon_reviews",
      "comment": "Aggregates ratings and occurrence counts, sorted by frequency of negative sentiment",
      "query": "SELECT\n    product_id,\n    any(product_title),\n    avg(star_rating),\n    count() AS count\nFROM amazon.amazon_reviews\nWHERE position(review_body, 'awful') > 0\nGROUP BY product_id\nORDER BY count DESC\nLIMIT 50;",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "find-products-with-awful-in-reviews"
    },
    {
      "id": "VWSFKKJTECSIDPHNCYE5BP",
      "name": "Find products with 'awesome' in reviews",
      "group": "amazon_reviews",
      "comment": "Aggregates ratings and occurrence counts, sorted by frequency of positive sentiment",
      "query": "SELECT\n    product_id,\n    any(product_title),\n    avg(star_rating),\n    count() AS count\nFROM amazon.amazon_reviews\nWHERE position(review_body, 'awesome') > 0\nGROUP BY product_id\nORDER BY count DESC\nLIMIT 50;",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "find-products-with-awesome-in-reviews"
    },
    {
      "id": "GEOICKHUJBUXMYROUXJK7C",
      "name": "CPU/network utilization per web server",
      "group": "mgbench",
      "comment": "",
      "query": "-- Q1.1: What is the CPU/network utilization for each web server since midnight?\n\nSELECT machine_name,\n       MIN(cpu) AS cpu_min,\n       MAX(cpu) AS cpu_max,\n       AVG(cpu) AS cpu_avg,\n       MIN(net_in) AS net_in_min,\n       MAX(net_in) AS net_in_max,\n       AVG(net_in) AS net_in_avg,\n       MIN(net_out) AS net_out_min,\n       MAX(net_out) AS net_out_max,\n       AVG(net_out) AS net_out_avg\nFROM (\n  SELECT machine_name,\n         COALESCE(cpu_user, 0.0) AS cpu,\n         COALESCE(bytes_in, 0.0) AS net_in,\n         COALESCE(bytes_out, 0.0) AS net_out\n  FROM mgbench.logs1\n  WHERE machine_name IN ('anansi','aragog','urd')\n    AND log_time >= TIMESTAMP '2017-01-11 00:00:00'\n) AS r\nGROUP BY machine_name;",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "cpunetwork-utilization-per-web-server"
    },
    {
      "id": "FKYNODO5J9SFHP9K7VXJAZ",
      "name": "Machine offline in the past day",
      "group": "mgbench",
      "comment": "",
      "query": "-- Q1.2: Which computer lab machines have been offline in the past day?\n\nSELECT machine_name,\n       log_time\nFROM mgbench.logs1\nWHERE (machine_name LIKE 'cslab%' OR\n       machine_name LIKE 'mslab%')\n  AND load_one IS NULL\n  AND log_time >= TIMESTAMP '2017-01-10 00:00:00'\nORDER BY machine_name,\n         log_time;",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "machine-offline-in-the-past-day"
    },
    {
      "id": "EVVZPZ2K5VDLHJ69SNJH1L",
      "name": "Hourly average metrics for specific workstation last 10 days",
      "group": "mgbench",
      "comment": "",
      "query": "-- Q1.3: What are the hourly average metrics during the past 10 days for a specific workstation?\n\nSELECT dt,\n       hr,\n       AVG(load_fifteen) AS load_fifteen_avg,\n       AVG(load_five) AS load_five_avg,\n       AVG(load_one) AS load_one_avg,\n       AVG(mem_free) AS mem_free_avg,\n       AVG(swap_free) AS swap_free_avg\nFROM (\n  SELECT CAST(log_time AS DATE) AS dt,\n         EXTRACT(HOUR FROM log_time) AS hr,\n         load_fifteen,\n         load_five,\n         load_one,\n         mem_free,\n         swap_free\n  FROM mgbench.logs1\n  WHERE machine_name = 'babbage'\n    AND load_fifteen IS NOT NULL\n    AND load_five IS NOT NULL\n    AND load_one IS NOT NULL\n    AND mem_free IS NOT NULL\n    AND swap_free IS NOT NULL\n    AND log_time >= TIMESTAMP '2017-01-01 00:00:00'\n) AS r\nGROUP BY dt,\n         hr\nORDER BY dt,\n         hr;",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "hourly-average-metrics-for-specific-workstation-last-10-days"
    },
    {
      "id": "JMQOG4IV5ADD46Y6QBG4AO",
      "name": "Server blocked on disk I/O",
      "group": "mgbench",
      "comment": "",
      "query": "-- Q1.4: Over 1 month, how often was each server blocked on disk I/O?\n\nSELECT machine_name,\n       COUNT(*) AS spikes\nFROM mgbench.logs1\nWHERE machine_group = 'Servers'\n  AND cpu_wio > 0.99\n  AND log_time >= TIMESTAMP '2016-12-01 00:00:00'\n  AND log_time < TIMESTAMP '2017-01-01 00:00:00'\nGROUP BY machine_name\nORDER BY spikes DESC\nLIMIT 10;",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "server-blocked-on-disk-io"
    },
    {
      "id": "JJQ4TZJTB6HGD8YLEHPRCE",
      "name": "Low memory external VMs",
      "group": "mgbench",
      "comment": "",
      "query": "-- Q1.5: Which externally reachable VMs have run low on memory?\n\nSELECT machine_name,\n       dt,\n       MIN(mem_free) AS mem_free_min\nFROM (\n  SELECT machine_name,\n         CAST(log_time AS DATE) AS dt,\n         mem_free\n  FROM mgbench.logs1\n  WHERE machine_group = 'DMZ'\n    AND mem_free IS NOT NULL\n) AS r\nGROUP BY machine_name,\n         dt\nHAVING MIN(mem_free) < 10000\nORDER BY machine_name,\n         dt;",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "low-memory-external-vms"
    },
    {
      "id": "POIZV5NWCQXZ7P7CYNQFYD",
      "name": "Total hourly network traffic",
      "group": "mgbench",
      "comment": "",
      "query": "-- Q1.6: What is the total hourly network traffic across all file servers?\n\nSELECT dt,\n       hr,\n       SUM(net_in) AS net_in_sum,\n       SUM(net_out) AS net_out_sum,\n       SUM(net_in) + SUM(net_out) AS both_sum\nFROM (\n  SELECT CAST(log_time AS DATE) AS dt,\n         EXTRACT(HOUR FROM log_time) AS hr,\n         COALESCE(bytes_in, 0.0) / 1000000000.0 AS net_in,\n         COALESCE(bytes_out, 0.0) / 1000000000.0 AS net_out\n  FROM mgbench.logs1\n  WHERE machine_name IN ('allsorts','andes','bigred','blackjack','bonbon',\n      'cadbury','chiclets','cotton','crows','dove','fireball','hearts','huey',\n      'lindt','milkduds','milkyway','mnm','necco','nerds','orbit','peeps',\n      'poprocks','razzles','runts','smarties','smuggler','spree','stride',\n      'tootsie','trident','wrigley','york')\n) AS r\nGROUP BY dt,\n         hr\nORDER BY both_sum DESC\nLIMIT 10;",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "total-hourly-network-traffic"
    },
    {
      "id": "W82SBNYFW4KENKMAJ1S9T1",
      "name": "Requests cause server errors past 2 weeks",
      "group": "mgbench",
      "comment": "",
      "query": "-- Q2.1: Which requests have caused server errors within the past 2 weeks?\n\nSELECT *\nFROM mgbench.logs2\nWHERE status_code >= 500\n  AND log_time >= TIMESTAMP '2012-12-18 00:00:00'\nORDER BY log_time;",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "requests-cause-server-errors-past-2-weeks"
    },
    {
      "id": "T97TB1WVEF3V6WV1WTQSAX",
      "name": "Detect user password file leak",
      "group": "mgbench",
      "comment": "",
      "query": "-- Q2.2: During a specific 2-week period, was the user password file leaked?\n\nSELECT *\nFROM mgbench.logs2\nWHERE status_code >= 200\n  AND status_code < 300\n  AND request LIKE '%/etc/passwd%'\n  AND log_time >= TIMESTAMP '2012-05-06 00:00:00'\n  AND log_time < TIMESTAMP '2012-05-20 00:00:00';",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "detect-user-password-file-leak"
    },
    {
      "id": "F7QHSLYQGCGZDU1VFXKRMP",
      "name": "Avg path depth for top-level requests in the past month",
      "group": "mgbench",
      "comment": "",
      "query": "-- Q2.3: What was the average path depth for top-level requests in the past month?\n\nSELECT top_level,\n       AVG(LENGTH(request) - LENGTH(REPLACE(request, '/', ''))) AS depth_avg\nFROM (\n  SELECT SUBSTRING(request FROM 1 FOR len) AS top_level,\n         request\n  FROM (\n    SELECT POSITION(SUBSTRING(request FROM 2), '/') AS len,\n           request\n    FROM mgbench.logs2\n    WHERE status_code >= 200\n      AND status_code < 300\n      AND log_time >= TIMESTAMP '2012-12-01 00:00:00'\n  ) AS r\n  WHERE len > 0\n) AS s\nWHERE top_level IN ('/about','/courses','/degrees','/events',\n                    '/grad','/industry','/news','/people',\n                    '/publications','/research','/teaching','/ugrad')\nGROUP BY top_level\nORDER BY top_level;",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "avg-path-depth-for-top-level-requests-in-the-past-month"
    },
    {
      "id": "T25HUSKE3NRV5VFCNMVXAD",
      "name": "Detect clients with excessive number of requests",
      "group": "mgbench",
      "comment": "",
      "query": "-- Q2.4: During the last 3 months, which clients have made an excessive number of requests?\n\nSELECT client_ip,\n       COUNT(*) AS num_requests\nFROM mgbench.logs2\nWHERE log_time >= TIMESTAMP '2012-10-01 00:00:00'\nGROUP BY client_ip\nHAVING COUNT(*) >= 100000\nORDER BY num_requests DESC;",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "detect-clients-with-excessive-number-of-requests"
    },
    {
      "id": "UTDAJTYXHW9CJSAIA1LXOU",
      "name": "Daily unique visitors",
      "group": "mgbench",
      "comment": "",
      "query": "-- Q2.5: What are the daily unique visitors?\n\nSELECT dt,\n       COUNT(DISTINCT client_ip)\nFROM (\n  SELECT CAST(log_time AS DATE) AS dt,\n         client_ip\n  FROM mgbench.logs2\n) AS r\nGROUP BY dt\nORDER BY dt;",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "daily-unique-visitors"
    },
    {
      "id": "XUZAXVAMUELHY3JKXZJ57X",
      "name": "Avg and max data transfer rates",
      "group": "mgbench",
      "comment": "",
      "query": "-- Q2.6: What are the average and maximum data transfer rates (Gbps)?\n\nSELECT AVG(transfer) / 125000000.0 AS transfer_avg,\n       MAX(transfer) / 125000000.0 AS transfer_max\nFROM (\n  SELECT log_time,\n         SUM(object_size) AS transfer\n  FROM mgbench.logs2\n  GROUP BY log_time\n) AS r;",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "avg-and-max-data-transfer-rates"
    },
    {
      "id": "8NYJBXCD4FEHS1UMGQVI9G",
      "name": "Detect freezing indoor temperature",
      "group": "mgbench",
      "comment": "",
      "query": "-- Q3.1: Did the indoor temperature reach freezing over the weekend?\n\nSELECT *\nFROM mgbench.logs3\nWHERE event_type = 'temperature'\n  AND event_value <= 32.0\n  AND log_time >= '2019-11-29 17:00:00.000';",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "detect-freezing-indoor-temperature"
    },
    {
      "id": "ARQ8M5MAJQAGWL2S5QKMVD",
      "name": "Last 6 months number of door openings",
      "group": "mgbench",
      "comment": "",
      "query": "-- Q3.4: Over the past 6 months, how frequently were each door opened?\n\nSELECT device_name,\n       device_floor,\n       COUNT(*) AS ct\nFROM mgbench.logs3\nWHERE event_type = 'door_open'\n  AND log_time >= '2019-06-01 00:00:00.000'\nGROUP BY device_name,\n         device_floor\nORDER BY ct DESC;",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "last-6-months-number-of-door-openings"
    },
    {
      "id": "WAMYYT2EUF2J9LEXVZ2HQW",
      "name": "Find where temperature variations occur in winter and summer",
      "group": "mgbench",
      "comment": "",
      "query": "\n\n-- Q3.5: Where in the building do large temperature variations occur in winter and summer?\n\nWITH temperature AS (\n  SELECT dt,\n         device_name,\n         device_type,\n         device_floor\n  FROM (\n    SELECT dt,\n           hr,\n           device_name,\n           device_type,\n           device_floor,\n           AVG(event_value) AS temperature_hourly_avg\n    FROM (\n      SELECT CAST(log_time AS DATE) AS dt,\n             EXTRACT(HOUR FROM log_time) AS hr,\n             device_name,\n             device_type,\n             device_floor,\n             event_value\n      FROM mgbench.logs3\n      WHERE event_type = 'temperature'\n    ) AS r\n    GROUP BY dt,\n             hr,\n             device_name,\n             device_type,\n             device_floor\n  ) AS s\n  GROUP BY dt,\n           device_name,\n           device_type,\n           device_floor\n  HAVING MAX(temperature_hourly_avg) - MIN(temperature_hourly_avg) >= 25.0\n)\nSELECT DISTINCT device_name,\n       device_type,\n       device_floor,\n       'WINTER'\nFROM temperature\nWHERE dt >= DATE '2018-12-01'\n  AND dt < DATE '2019-03-01'\nUNION DISTINCT\nSELECT DISTINCT device_name,\n       device_type,\n       device_floor,\n       'SUMMER'\nFROM temperature\nWHERE dt >= DATE '2019-06-01'\n  AND dt < DATE '2019-09-01';",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "find-where-temperature-variations-occur-in-winter-and-summer"
    },
    {
      "id": "VQWGJ3Y4A5CBGWXEQBDEJL",
      "name": "Monthly power consumption metrics per device category",
      "group": "mgbench",
      "comment": "",
      "query": "-- Q3.6: For each device category, what are the monthly power consumption metrics?\n\nSELECT yr,\n       mo,\n       SUM(coffee_hourly_avg) AS coffee_monthly_sum,\n       AVG(coffee_hourly_avg) AS coffee_monthly_avg,\n       SUM(printer_hourly_avg) AS printer_monthly_sum,\n       AVG(printer_hourly_avg) AS printer_monthly_avg,\n       SUM(projector_hourly_avg) AS projector_monthly_sum,\n       AVG(projector_hourly_avg) AS projector_monthly_avg,\n       SUM(vending_hourly_avg) AS vending_monthly_sum,\n       AVG(vending_hourly_avg) AS vending_monthly_avg\nFROM (\n  SELECT dt,\n         yr,\n         mo,\n         hr,\n         AVG(coffee) AS coffee_hourly_avg,\n         AVG(printer) AS printer_hourly_avg,\n         AVG(projector) AS projector_hourly_avg,\n         AVG(vending) AS vending_hourly_avg\n  FROM (\n    SELECT CAST(log_time AS DATE) AS dt,\n           EXTRACT(YEAR FROM log_time) AS yr,\n           EXTRACT(MONTH FROM log_time) AS mo,\n           EXTRACT(HOUR FROM log_time) AS hr,\n           CASE WHEN device_name LIKE 'coffee%' THEN event_value END AS coffee,\n           CASE WHEN device_name LIKE 'printer%' THEN event_value END AS printer,\n           CASE WHEN device_name LIKE 'projector%' THEN event_value END AS projector,\n           CASE WHEN device_name LIKE 'vending%' THEN event_value END AS vending\n    FROM mgbench.logs3\n    WHERE device_type = 'meter'\n  ) AS r\n  GROUP BY dt,\n           yr,\n           mo,\n           hr\n) AS s\nGROUP BY yr,\n         mo\nORDER BY yr,\n         mo;",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "monthly-power-consumption-metrics-per-device-category"
    },
    {
      "id": "UV8M4MAGS2PWAUOAYAAARM",
      "name": "Cell towers by MCC",
      "group": "geo",
      "comment": "Lists top 10 mobile country codes by infrastructure density",
      "query": "SELECT mcc, count() FROM geo.cell_towers GROUP BY mcc ORDER BY count() DESC LIMIT 10",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "cell-towers-by-mcc"
    },
    {
      "id": "VYCIREOC1QSOPKUVANIUZK",
      "name": "Cell towers by type",
      "group": "geo",
      "comment": "Summarizes transmission technology distribution across infrastructure",
      "query": "SELECT radio, count() AS c FROM geo.cell_towers GROUP BY radio ORDER BY c DESC",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "cell-towers-by-type"
    },
    {
      "id": "2F9AQGB8CJR2DZ7ZAPY73Y",
      "name": "Number of cell towers in Moscow",
      "group": "geo",
      "comment": "Counts infrastructure points within city polygon boundaries",
      "query": "SELECT count() FROM geo.cell_towers\nWHERE pointInPolygon((lon, lat), (SELECT * FROM geo.moscow))",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "number-of-cell-towers-in-moscow"
    },
    {
      "id": "CMRQXM78SRPEZ9XU65GTQM",
      "name": "Number of rows",
      "group": "covid",
      "comment": "Returns total dataset entries in human-readable format",
      "query": "SELECT formatReadableQuantity(count())\nFROM covid.covid19;",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "number-of-rows"
    },
    {
      "id": "356YYFGKQ2ED9OKIPNSRMC",
      "name": "Number of cases confirmed",
      "group": "covid",
      "comment": "Aggregates total infections in human-readable format",
      "query": "SELECT formatReadableQuantity(sum(new_confirmed))\nFROM covid.covid19;",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "number-of-cases-confirmed"
    },
    {
      "id": "OBVBSACGRIAMJI16AI42VM",
      "name": "Daily averages of new case",
      "group": "covid",
      "comment": "Calculates 5-day rolling mean using window functions by location",
      "query": "SELECT\n   AVG(new_confirmed) OVER (PARTITION BY location_key ORDER BY date ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS cases_smoothed,\n   new_confirmed,\n   location_key,\n   date\nFROM covid.covid19;",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "daily-averages-of-new-case"
    },
    {
      "id": "8KWGRUKIYUTKWVQW4CBYDP",
      "name": "Lag new cases each day for US-DC",
      "group": "covid",
      "comment": "Retrieves most recent mortality and infection data per region",
      "query": "WITH latest_deaths_data AS\n   ( SELECT location_key,\n            date,\n            new_deceased,\n            new_confirmed,\n            ROW_NUMBER() OVER (PARTITION BY location_key ORDER BY date DESC) as rn\n     FROM covid.covid19)\nSELECT location_key,\n       date,\n       new_deceased,\n       new_confirmed,\n       rn\nFROM latest_deaths_data\nWHERE rn=1;",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "lag-new-cases-each-day-for-us-dc"
    },
    {
      "id": "2MENTDSVJQML2VRCO2UTII",
      "name": "Percentage new cases change per day",
      "group": "covid",
      "comment": "Analyzes day-over-day trends with directional indicators for DC region",
      "query": "WITH confirmed_lag AS (\n  SELECT\n    *,\n    lagInFrame(new_confirmed) OVER(\n      PARTITION BY location_key\n      ORDER BY date\n    ) AS confirmed_previous_day\n  FROM covid.covid19\n),\nconfirmed_percent_change AS (\n  SELECT\n    *,\n    COALESCE(ROUND((new_confirmed - confirmed_previous_day) / confirmed_previous_day * 100), 0) AS percent_change\n  FROM confirmed_lag\n)\nSELECT\n  date,\n  new_confirmed,\n  percent_change,\n  CASE\n    WHEN percent_change > 0 THEN 'increase'\n    WHEN percent_change = 0 THEN 'no change'\n    ELSE 'decrease'\n  END AS trend\nFROM confirmed_percent_change\nWHERE location_key = 'US_DC';",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "percentage-new-cases-change-per-day"
    },
    {
      "id": "HQXNQZE26Z1QWYP9KC76ML",
      "name": "Most common ingredients",
      "group": "food",
      "comment": "Extracts and ranks named entities by frequency from recipe database",
      "query": "SELECT\n    arrayJoin(NER) AS k,\n    count() AS c\nFROM food.recipes\nGROUP BY k\nORDER BY c DESC\nLIMIT 50",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "most-common-ingredients"
    },
    {
      "id": "1MXMHASDLEQIP4P1D1STND",
      "name": "Usage per machine",
      "group": "mgbench",
      "comment": "",
      "query": "SELECT machine_name,\n       MIN(cpu) AS cpu_min,\n       MAX(cpu) AS cpu_max,\n       AVG(cpu) AS cpu_avg,\n       MIN(net_in) AS net_in_min,\n       MAX(net_in) AS net_in_max,\n       AVG(net_in) AS net_in_avg,\n       MIN(net_out) AS net_out_min,\n       MAX(net_out) AS net_out_max,\n       AVG(net_out) AS net_out_avg\nFROM (\n  SELECT machine_name,\n         COALESCE(cpu_user, 0.0) AS cpu,\n         COALESCE(bytes_in, 0.0) AS net_in,\n         COALESCE(bytes_out, 0.0) AS net_out\n  FROM mgbench.logs1\n  WHERE machine_name IN ('anansi','aragog','urd')\n    AND log_time >= TIMESTAMP '2017-01-11 00:00:00'\n) AS r\nGROUP BY machine_name",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "usage-per-machine"
    },
    {
      "id": "478GCPU7LRTSZJBNY3EJT3",
      "name": "License types per dependencies",
      "group": "default",
      "comment": "Identifies the license types associated with each library dependency in the system.",
      "query": "SELECT library_name, license_type, license_path FROM system.licenses ORDER BY library_name COLLATE 'en'",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "license-types-per-dependencies"
    },
    {
      "id": "7YB4Q9HMD22ZV7VQWBFREA",
      "name": "Number cumulative contributors",
      "group": "git",
      "comment": "Calculate the cumulative number of unique contributors to \"ClickHouse\" Pull Requests over time.",
      "query": "WITH states AS\n    (\n        SELECT\n            month,\n            uniqState(actor_login) AS uniq_users\n        FROM git.github_events\n        WHERE (repo_name LIKE 'ClickHouse%') AND (event_type = 'PullRequestEvent')\n        GROUP BY toStartOfMonth(created_at) AS month\n        ORDER BY month ASC\n    )\nSELECT\n    month,\n    uniqMerge(uniq_users) OVER (ORDER BY month ASC) AS cul_users\nFROM states",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "number-cumulative-contributors"
    },
    {
      "id": "KIZPJXDYG1QB6DQDFSAVIE",
      "name": "Number of unique and cumulative contributors",
      "group": "git",
      "comment": "Calculates the unique and cumulative number of contributors to ClickHouse via pull request events.",
      "query": "SELECT\n    month,\n    arrayUniq(cul_users) AS cul_users,\n    arrayUniq(uniq_users) AS uniq_users\nFROM\n(\n    SELECT\n        month,\n        groupArrayDistinctArray(uniq_users) OVER (ORDER BY month ASC) AS cul_users,\n        groupArrayDistinct(actor_login) AS uniq_users\n    FROM git.github_events\n    WHERE (repo_name LIKE 'ClickHouse%') AND (event_type = 'PullRequestEvent')\n    GROUP BY toStartOfMonth(created_at) AS month\n    ORDER BY month ASC\n)\n",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "number-of-unique-and-cumulative-contributors"
    },
    {
      "id": "DCQPNPAIMAQXRLHYURLKVJ",
      "name": "Show tables",
      "group": "git",
      "comment": "Displays a list of tables in the specified Git database.",
      "query": "SHOW TABLES IN git",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "show-tables"
    },
    {
      "id": "9YHSK62D6WHC5ARCBOJEO7",
      "name": "Show events from dbt-clickhouse repo",
      "group": "git",
      "comment": "This query retrieves all GitHub events associated with the dbt-clickhouse repository, organized in chronological order.",
      "query": "SELECT * FROM git.github_events WHERE repo_name LIKE '%/dbt-clickhouse' ORDER BY created_at ASC",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "show-events-from-dbt-clickhouse-repo"
    },
    {
      "id": "1IXC5PU4QCXSH5DHGQRU6F",
      "name": "Show events from Clickhouse/dbt-clickhouse repo",
      "group": "git",
      "comment": "Display the chronological list of GitHub events from the ClickHouse/dbt-clickhouse repository.",
      "query": "SELECT * FROM git.github_events WHERE repo_name LIKE 'ClickHouse/dbt-clickhouse' ORDER BY created_at ASC",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "show-events-from-clickhousedbt-clickhouse-repo"
    },
    {
      "id": "JAUTYJJXBOSATPJD9B624X",
      "name": "Select 1 event",
      "group": "git",
      "comment": "Selects all columns from the 'github_events' table but limits the output to a single entry for analysis or testing purposes.",
      "query": "SELECT * FROM git.github_events LIMIT 1",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "select-1-event"
    },
    {
      "id": "1BSPOBWAT6PUOCYXYNPNML",
      "name": "Compute if commit days are consecutive",
      "group": "git",
      "comment": "Checks if commit days by each author are consecutive by calculating the difference between consecutive commit days.",
      "query": "SELECT\n    author,\n    toDate(day) as day,\n    any(day) OVER (PARTITION BY author ORDER BY day ASC ROWS BETWEEN 1 PRECEDING AND CURRENT ROW) AS previous_commit,\n    dateDiff('day', previous_commit, day) AS day_diff,\n    if(day_diff = 1, 1, 0) AS consecutive\nFROM\n(\n   SELECT author, toStartOfDay(time) AS day FROM git.clickhouse_commits GROUP BY author, day ORDER BY author ASC, day ASC\n)\nLIMIT 10",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "compute-if-commit-days-are-consecutive"
    },
    {
      "id": "WSHUEPJP9TNJUH7QITWWOR",
      "name": "Average and median rewrite time across all files",
      "group": "git",
      "comment": "Calculates average and median time between significant rewrites of files, filtering for modifications meeting size and alteration percentage criteria.",
      "query": "WITH\n    changes AS\n    (\n        SELECT\n            path,\n            commit_hash,\n            max_time,\n            type,\n            num_added,\n            num_deleted,\n            sum(num_added - num_deleted) OVER (PARTITION BY path ORDER BY max_time ASC) AS current_size,\n            if(current_size > 0, num_added / current_size, 0) AS percent_add,\n            if(current_size > 0, num_deleted / current_size, 0) AS percent_delete\n        FROM\n        (\n            SELECT\n                path,\n                max(time) AS max_time,\n                commit_hash,\n                any(lines_added) AS num_added,\n                any(lines_deleted) AS num_deleted,\n                any(change_type) AS type\n            FROM git.clickhouse_file_changes\n            WHERE (change_type IN ('Add', 'Modify')) AND (file_extension IN ('h', 'cpp', 'sql'))\n            GROUP BY\n                path,\n                commit_hash\n            ORDER BY\n                path ASC,\n                max_time ASC\n        )\n    ),\n    rewrites AS\n    (\n        SELECT\n            *,\n            any(max_time) OVER (PARTITION BY path ORDER BY max_time ASC ROWS BETWEEN 1 PRECEDING AND CURRENT ROW) AS previous_rewrite,\n            dateDiff('day', previous_rewrite, max_time) AS rewrite_days\n        FROM changes\n        WHERE (type = 'Modify') AND (percent_add >= 0.5) AND (percent_delete >= 0.5) AND (current_size > 50)\n    )\nSELECT\n    avgIf(rewrite_days, rewrite_days > 0) AS avg_rewrite_time,\n    quantilesTimingIf(0.5)(rewrite_days, rewrite_days > 0) AS half_life\nFROM rewrites",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "average-and-median-rewrite-time-across-all-files"
    },
    {
      "id": "COAZRFX2YFULDBXRQTCQ1S",
      "name": "All commit messages for specific topic",
      "group": "git",
      "comment": "Lists the last 10 commits with changes to the 'src/Storages/StorageReplicatedMergeTree.cpp' file, displaying details like commit hash, author, lines added/deleted, and commit message in descending order of time.",
      "query": "SELECT\n    time,\n    substring(commit_hash, 1, 11) AS commit,\n    change_type,\n    author,\n    path,\n    old_path,\n    lines_added,\n    lines_deleted,\n    commit_message\nFROM git.clickhouse_file_changes\nWHERE path = 'src/Storages/StorageReplicatedMergeTree.cpp'\nORDER BY time DESC\nLIMIT 10",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "all-commit-messages-for-specific-topic"
    },
    {
      "id": "5AUH3VVENJ6CFEFWDKW5ZH",
      "name": "Author with commits for the most number of consecutive days",
      "group": "git",
      "comment": "Identifies and lists authors who have made commits on the most consecutive days, ordered by author and day.",
      "query": "SELECT\n    author,\n    toStartOfDay(time) AS day\nFROM git.clickhouse_commits\nGROUP BY\n    author,\n    day\nORDER BY\n    author ASC,\n    day ASC\nLIMIT 10",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "author-with-commits-for-the-most-number-of-consecutive-days"
    },
    {
      "id": "7ARCDLHQCESYBP3INDPTVO",
      "name": "Get one hash and time",
      "group": "git",
      "comment": "Retrieves the most recent commit hash and its timestamp from the ClickHouse Git commit history, sorted in descending order by time.",
      "query": "SELECT hash, time FROM git.clickhouse_commits ORDER BY time DESC LIMIT 1",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "get-one-hash-and-time"
    },
    {
      "id": "AKS9SYLARFMZCHGAAQNEBN",
      "name": "Review all line changes for specific topic",
      "group": "git",
      "comment": "Analyzes recent line changes with commit hashes and author information in a specific file of a Git repository.",
      "query": "SELECT\n    time,\n    substring(commit_hash, 1, 11) AS commit,\n    sign,\n    line_number_old,\n    line_number_new,\n    author,\n    line\nFROM git.clickhouse_line_changes\nWHERE path = 'src/Storages/StorageReplicatedMergeTree.cpp'\nORDER BY line_number_new ASC\nLIMIT 10",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "review-all-line-changes-for-specific-topic"
    },
    {
      "id": "2HNFWPCFWEEY92WTAPMA7W",
      "name": "Find current active files",
      "group": "git",
      "comment": "Identifies the current active files in the git repository, excluding specific directories, by checking the latest file changes and filtering out files marked with a change type of 2.",
      "query": "SELECT path\nFROM\n(\n    SELECT\n        old_path AS path,\n        max(time) AS last_time,\n        2 AS change_type\n    FROM git.clickhouse_file_changes\n    GROUP BY old_path\n    UNION ALL\n    SELECT\n        path,\n        max(time) AS last_time,\n        argMax(change_type, time) AS change_type\n    FROM git.clickhouse_file_changes\n    GROUP BY path\n)\nGROUP BY path\nHAVING (argMax(change_type, last_time) != 2) AND NOT match(path, '(^dbms/)|(^libs/)|(^tests/testflows/)|(^programs/server/store/)') ORDER BY path\nLIMIT 10",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "find-current-active-files"
    },
    {
      "id": "1OXCKMOH2JVMSHD3NS2WW6",
      "name": "Find number of active files",
      "group": "git",
      "comment": "Calculates the unique count of current active file paths in a repository, excluding specific directories.",
      "query": "SELECT uniq(path)\nFROM\n(\n    SELECT path\n    FROM\n    (\n        SELECT\n            old_path AS path,\n            max(time) AS last_time,\n            2 AS change_type\n        FROM git.clickhouse_file_changes\n        GROUP BY old_path\n        UNION ALL\n        SELECT\n            path,\n            max(time) AS last_time,\n            argMax(change_type, time) AS change_type\n        FROM git.clickhouse_file_changes\n        GROUP BY path\n    )\n    GROUP BY path\n    HAVING (argMax(change_type, last_time) != 2) AND NOT match(path, '(^dbms/)|(^libs/)|(^tests/testflows/)|(^programs/server/store/)') ORDER BY path\n)",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "find-number-of-active-files"
    },
    {
      "id": "SCXWMR9GBMJ9UNZYQXQBFA",
      "name": "List of changes for renamed file",
      "group": "git",
      "comment": "Extracts the change type, paths, timestamp, and commit hash for modifications related to a specific file, considering both its current and previous names.",
      "query": "SELECT\n      change_type,\n      path,\n      old_path,\n      time,\n      commit_hash\n  FROM git.clickhouse_file_changes\n  WHERE (path = 'src/Functions/geometryFromColumn.h') OR (old_path = 'src/Functions/geometryFromColumn.h')\n",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "list-of-changes-for-renamed-file"
    },
    {
      "id": "MHXPSBNPTDMJYR3OYSXVR7",
      "name": "List files with most modifications",
      "group": "git",
      "comment": "Identifies the top 10 files with the most modifications, excluding certain directories and considering only specific file extensions, by summing additions and deletions from their version history.",
      "query": "WITH current_files AS\n    (\n        SELECT path\n        FROM\n        (\n            SELECT\n                old_path AS path,\n                max(time) AS last_time,\n                2 AS change_type\n            FROM git.clickhouse_file_changes\n            GROUP BY old_path\n            UNION ALL\n            SELECT\n                path,\n                max(time) AS last_time,\n                argMax(change_type, time) AS change_type\n            FROM git.clickhouse_file_changes\n            GROUP BY path\n        )\n        GROUP BY path\n        HAVING (argMax(change_type, last_time) != 2) AND (NOT match(path, '(^dbms/)|(^libs/)|(^tests/testflows/)|(^programs/server/store/)'))\n        ORDER BY path ASC\n    )\nSELECT\n    path,\n    sum(lines_added) + sum(lines_deleted) AS modifications\nFROM git.clickhouse_file_changes\nWHERE (path IN (current_files)) AND (file_extension IN ('h', 'cpp', 'sql'))\nGROUP BY path\nORDER BY modifications DESC\nLIMIT 10",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "list-files-with-most-modifications"
    },
    {
      "id": "GED2STFSYJDRAA59H8RLIV",
      "name": "Number of commits per week day",
      "group": "geo",
      "comment": "Analyzes the total number of commits for each day of the week from the git.clickhouse_commits database.",
      "query": "SELECT\n    day_of_week,\n    count() AS c\nFROM git.clickhouse_commits\nGROUP BY dayOfWeek(time) AS day_of_week",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "number-of-commits-per-week-day"
    },
    {
      "id": "REZRXDVU7CAWT5WKNJSTNY",
      "name": "History of subdirectory/file",
      "group": "git",
      "comment": "Analyzes weekly changes in lines added, lines deleted, commits, and authors for the 'src/Storages' subdirectory in a Git repository.",
      "query": "SELECT\n    week,\n    sum(lines_added) AS lines_added,\n    sum(lines_deleted) AS lines_deleted,\n    uniq(commit_hash) AS num_commits,\n    uniq(author) AS authors\nFROM git.clickhouse_file_changes\nWHERE path LIKE 'src/Storages%'\nGROUP BY toStartOfWeek(time) AS week\nORDER BY week ASC\nLIMIT 10\n",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "history-of-subdirectoryfile"
    },
    {
      "id": "CYQFNQNK9TAMPU2OZ8KG5Y",
      "name": "List files with maximum number of authors",
      "group": "git",
      "comment": "Identifies the top 10 files in a repository with the highest number of unique authors, filtering out certain paths and files that have been deleted.",
      "query": "WITH current_files AS\n    (\n        SELECT path\n        FROM\n        (\n            SELECT\n                old_path AS path,\n                max(time) AS last_time,\n                2 AS change_type\n            FROM git.clickhouse_file_changes\n            GROUP BY old_path\n            UNION ALL\n            SELECT\n                path,\n                max(time) AS last_time,\n                argMax(change_type, time) AS change_type\n            FROM git.clickhouse_file_changes\n            GROUP BY path\n        )\n        GROUP BY path\n        HAVING (argMax(change_type, last_time) != 2) AND (NOT match(path, '(^dbms/)|(^libs/)|(^tests/testflows/)|(^programs/server/store/)'))\n        ORDER BY path ASC\n    )\nSELECT\n    path,\n    uniq(author) AS num_authors\nFROM git.clickhouse_file_changes\nWHERE path IN (current_files)\nGROUP BY path\nORDER BY num_authors DESC\nLIMIT 10",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "list-files-with-maximum-number-of-authors"
    },
    {
      "id": "VWPBPGRZVGTHOCQYWNQZNT",
      "name": "Oldest lines of code in the repository",
      "group": "git",
      "comment": "Identifies the oldest unchanged lines of code in the repository by excluding recent file changes and specific directories.",
      "query": "WITH current_files AS\n    (\n        SELECT path\n        FROM\n        (\n            SELECT\n                old_path AS path,\n                max(time) AS last_time,\n                2 AS change_type\n            FROM git.clickhouse_file_changes\n            GROUP BY old_path\n            UNION ALL\n            SELECT\n                path,\n                max(time) AS last_time,\n                argMax(change_type, time) AS change_type\n            FROM git.clickhouse_file_changes\n            GROUP BY path\n        )\n        GROUP BY path\n        HAVING (argMax(change_type, last_time) != 2) AND (NOT match(path, '(^dbms/)|(^libs/)|(^tests/testflows/)|(^programs/server/store/)'))\n        ORDER BY path ASC\n    )\nSELECT\n    any(path) AS file_path,\n    line,\n    max(time) AS latest_change,\n    any(file_change_type)\nFROM git.clickhouse_line_changes\nWHERE path IN (current_files)\nGROUP BY line\nORDER BY latest_change ASC\nLIMIT 10",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "oldest-lines-of-code-in-the-repository"
    },
    {
      "id": "EONJVEYDCODCUZUF8SNB4Y",
      "name": "Files with longest history",
      "group": "git",
      "comment": "Identify the top 10 files with the most changes, excluding certain directories, and display the latest modification time.",
      "query": "WITH current_files AS\n    (\n        SELECT path\n        FROM\n        (\n            SELECT\n                old_path AS path,\n                max(time) AS last_time,\n                2 AS change_type\n            FROM git.clickhouse_file_changes\n            GROUP BY old_path\n            UNION ALL\n            SELECT\n                path,\n                max(time) AS last_time,\n                argMax(change_type, time) AS change_type\n            FROM git.clickhouse_file_changes\n            GROUP BY path\n        )\n        GROUP BY path\n        HAVING (argMax(change_type, last_time) != 2) AND (NOT match(path, '(^dbms/)|(^libs/)|(^tests/testflows/)|(^programs/server/store/)'))\n        ORDER BY path ASC\n    )\nSELECT\n    count() AS c,\n    path,\n    max(time) AS latest_change\nFROM git.clickhouse_file_changes\nWHERE path IN (current_files)\nGROUP BY path\nORDER BY c DESC\nLIMIT 10",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "files-with-longest-history"
    },
    {
      "id": "BA4RZUXUHNQBH9YK7F2T9J",
      "name": "Distribution of contribution per day of the month",
      "group": "git",
      "comment": "Analyzes the ratio of documentation to code contributions by day of the month, visualizing the percentage of documentation contributions in a line chart.",
      "query": "SELECT\n    day,\n    bar(docs_ratio * 1000, 0, 100, 100) AS bar\nFROM\n(\n    SELECT\n        day,\n        countIf(file_extension IN ('h', 'cpp', 'sql')) AS code,\n        countIf(file_extension = 'md') AS docs,\n        docs / (code + docs) AS docs_ratio\n    FROM git.clickhouse_line_changes\n    WHERE (sign = 1) AND (file_extension IN ('h', 'cpp', 'sql', 'md'))\n    GROUP BY dayOfMonth(time) AS day\n)",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "distribution-of-contribution-per-day-of-the-month"
    },
    {
      "id": "MT8WBABUKYBYSBA78W5TML",
      "name": "Authors with the most diverse impact",
      "group": "git",
      "comment": "Identifies top 10 authors with the broadest influence based on their changes across unique file paths in specific languages.",
      "query": "SELECT\n    author,\n    uniq(path) AS num_files\nFROM git.clickhouse_file_changes\nWHERE (change_type IN ('Add', 'Modify')) AND (file_extension IN ('h', 'cpp', 'sql'))\nGROUP BY author\nORDER BY num_files DESC\nLIMIT 10",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "authors-with-the-most-diverse-impact"
    },
    {
      "id": "4Q3D67FWRIVWTY8EIDDE5U",
      "name": "Authors with most recent diverse contribution",
      "group": "git",
      "comment": "Identifies the top authors with the largest number of recent unique file contributions across 'h', 'cpp', and 'sql' extensions in the git repository.",
      "query": "SELECT\n    author,\n    sum(num_files_commit) AS num_files\nFROM\n(\n    SELECT\n        author,\n        commit_hash,\n        uniq(path) AS num_files_commit,\n        max(time) AS commit_time\n    FROM git.clickhouse_file_changes\n    WHERE (change_type IN ('Add', 'Modify')) AND (file_extension IN ('h', 'cpp', 'sql'))\n    GROUP BY\n        author,\n        commit_hash\n    ORDER BY\n        author ASC,\n        commit_time DESC\n    LIMIT 3 BY author\n)\nGROUP BY author\nORDER BY num_files DESC\nLIMIT 10",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "authors-with-most-recent-diverse-contribution"
    },
    {
      "id": "OKGZBACRHVGCRAGCZAJKMF",
      "name": "Favorite files for an author",
      "group": "git",
      "comment": "Identifies top 10 frequently changed files by a specific author, excluding certain directories, using recent changes data.",
      "query": "WITH current_files AS\n    (\n        SELECT path\n        FROM\n        (\n            SELECT\n                old_path AS path,\n                max(time) AS last_time,\n                2 AS change_type\n            FROM git.clickhouse_file_changes\n            GROUP BY old_path\n            UNION ALL\n            SELECT\n                path,\n                max(time) AS last_time,\n                argMax(change_type, time) AS change_type\n            FROM git.clickhouse_file_changes\n            GROUP BY path\n        )\n        GROUP BY path\n        HAVING (argMax(change_type, last_time) != 2) AND (NOT match(path, '(^dbms/)|(^libs/)|(^tests/testflows/)|(^programs/server/store/)'))\n        ORDER BY path ASC\n    )\nSELECT\n    path,\n    count() AS c\nFROM git.clickhouse_file_changes\nWHERE (author = 'Alexey Milovidov') AND (path IN (current_files))\nGROUP BY path\nORDER BY c DESC\nLIMIT 10",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "favorite-files-for-an-author"
    },
    {
      "id": "P9PBDZGOSVTKXEXU73ZNAJ",
      "name": "Favorite code files for an author",
      "group": "git",
      "comment": "Identifies the top 10 most frequently modified code files by author Alexey Milovidov with specific extensions, ordered by changes.",
      "query": "SELECT\n    base,\n    count() AS c\nFROM git.clickhouse_file_changes\nWHERE (author = 'Alexey Milovidov') AND (file_extension IN ('h', 'cpp', 'sql'))\nGROUP BY basename(path) AS base\nORDER BY c DESC\nLIMIT 10",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "favorite-code-files-for-an-author"
    },
    {
      "id": "PVSDOHZYUMRDDUZFEYJC7J",
      "name": "Largest files",
      "group": "git",
      "comment": "Identifies the top 10 current files in a project by line-to-author ratio, excluding certain directories and changes, based on historical Git changes data.",
      "query": "WITH current_files AS\n    (\n        SELECT path\n        FROM\n        (\n            SELECT\n                old_path AS path,\n                max(time) AS last_time,\n                2 AS change_type\n            FROM git.clickhouse_file_changes\n            GROUP BY old_path\n            UNION ALL\n            SELECT\n                path,\n                max(time) AS last_time,\n                argMax(change_type, time) AS change_type\n            FROM git.clickhouse_file_changes\n            GROUP BY path\n        )\n        GROUP BY path\n        HAVING (argMax(change_type, last_time) != 2) AND (NOT match(path, '(^dbms/)|(^libs/)|(^tests/testflows/)|(^programs/server/store/)'))\n        ORDER BY path ASC\n    )\nSELECT\n    path,\n    sum(lines_added) - sum(lines_deleted) AS num_lines,\n    uniqExact(author) AS num_authors,\n    num_lines / num_authors AS lines_author_ratio\nFROM git.clickhouse_file_changes\nWHERE path IN (current_files)\nGROUP BY path\nORDER BY lines_author_ratio DESC\nLIMIT 10",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "largest-files"
    },
    {
      "id": "BZHGWUIZMPZZUHS5XRBK2M",
      "name": "Largest code files",
      "group": "git",
      "comment": "Identify the top 10 largest code files by lines per author ratio in a Git repository, excluding certain paths and files with specific extensions.",
      "query": "WITH current_files AS\n    (\n        SELECT path\n        FROM\n        (\n            SELECT\n                old_path AS path,\n                max(time) AS last_time,\n                2 AS change_type\n            FROM git.clickhouse_file_changes\n            GROUP BY old_path\n            UNION ALL\n            SELECT\n                path,\n                max(time) AS last_time,\n                argMax(change_type, time) AS change_type\n            FROM git.clickhouse_file_changes\n            GROUP BY path\n        )\n        GROUP BY path\n        HAVING (argMax(change_type, last_time) != 2) AND (NOT match(path, '(^dbms/)|(^libs/)|(^tests/testflows/)|(^programs/server/store/)'))\n        ORDER BY path ASC\n    )\nSELECT\n    path,\n    sum(lines_added) - sum(lines_deleted) AS num_lines,\n    uniqExact(author) AS num_authors,\n    num_lines / num_authors AS lines_author_ratio\nFROM git.clickhouse_file_changes\nWHERE (path IN (current_files)) AND (file_extension IN ('h', 'cpp', 'sql'))\nGROUP BY path\nORDER BY lines_author_ratio DESC\nLIMIT 10",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "largest-code-files"
    },
    {
      "id": "RMHHZEDHFUCBGRQVQA2732",
      "name": "Largest recent code files with ",
      "group": "git",
      "comment": "Identifies the largest recent code files by calculating the lines-to-author ratio for files with specific extensions changed over the past year, excluding certain directories.",
      "query": "WITH current_files AS\n    (\n        SELECT path\n        FROM\n        (\n            SELECT\n                old_path AS path,\n                max(time) AS last_time,\n                2 AS change_type\n            FROM git.clickhouse_file_changes\n            GROUP BY old_path\n            UNION ALL\n            SELECT\n                path,\n                max(time) AS last_time,\n                argMax(change_type, time) AS change_type\n            FROM git.clickhouse_file_changes\n            GROUP BY path\n        )\n        GROUP BY path\n        HAVING (argMax(change_type, last_time) != 2) AND (NOT match(path, '(^dbms/)|(^libs/)|(^tests/testflows/)|(^programs/server/store/)'))\n        ORDER BY path ASC\n    )\nSELECT\n    min(time) AS min_date,\n    path,\n    sum(lines_added) - sum(lines_deleted) AS num_lines,\n    uniqExact(author) AS num_authors,\n    num_lines / num_authors AS lines_author_ratio\nFROM git.clickhouse_file_changes\nWHERE (path IN (current_files)) AND (file_extension IN ('h', 'cpp', 'sql'))\nGROUP BY path\nHAVING min_date <= (now() - toIntervalYear(1))\nORDER BY lines_author_ratio DESC\nLIMIT 10",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "largest-recent-code-files-with-"
    },
    {
      "id": "PF3KEMYG5CVLJGCFYQEGB1",
      "name": "Commits distribution by weekday",
      "group": "git",
      "comment": "Analyzes the distribution of Git commits by weekday, detailing the unique commit count and the total lines added or deleted for files in the 'src/Functions%' directory.",
      "query": "SELECT\n    dayOfWeek,\n    uniq(commit_hash) AS commits,\n    sum(lines_added) AS lines_added,\n    sum(lines_deleted) AS lines_deleted\nFROM git.clickhouse_file_changes\nWHERE path LIKE 'src/Functions%'\nGROUP BY toDayOfWeek(time) AS dayOfWeek",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "commits-distribution-by-weekday"
    },
    {
      "id": "Q4VDVKEGHHRBCUJHNCVTF1",
      "name": "Commits distribution by hour of the day",
      "group": "git",
      "comment": "Analyzes the frequency of unique commits and the total lines added or deleted by hour for files in the 'src/Functions' directory.",
      "query": "SELECT\n    hourOfDay,\n    uniq(commit_hash) AS commits,\n    sum(lines_added) AS lines_added,\n    sum(lines_deleted) AS lines_deleted\nFROM git.clickhouse_file_changes\nWHERE path LIKE 'src/Functions%'\nGROUP BY toHour(time) AS hourOfDay",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "commits-distribution-by-hour-of-the-day"
    },
    {
      "id": "9AZ8CENV8N91YGW7T6IB68",
      "name": "Charts commits distribution by hour of the day",
      "group": "git",
      "comment": "Visualizes the distribution of commits, lines added, and lines deleted by hour of the day for specific file paths.",
      "query": "SELECT\n    hourOfDay,\n    bar(commits, 0, 400, 50) AS commits,\n    bar(lines_added, 0, 30000, 50) AS lines_added,\n    bar(lines_deleted, 0, 15000, 50) AS lines_deleted\nFROM\n(\n    SELECT\n        hourOfDay,\n        uniq(commit_hash) AS commits,\n        sum(lines_added) AS lines_added,\n        sum(lines_deleted) AS lines_deleted\n    FROM git.clickhouse_file_changes\n    WHERE path LIKE 'src/Functions%'\n    GROUP BY toHour(time) AS hourOfDay\n)",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "charts-commits-distribution-by-hour-of-the-day"
    },
    {
      "id": "448O8GWAHY3EM6ZZ7AGLAM",
      "name": "Authors that tends to rewrite other authors code",
      "group": "git",
      "comment": "Identifies authors who frequently modify code written by others in C++ and header files within a repository.",
      "query": "SELECT\n    prev_author || '(a)' as add_author,\n    author  || '(d)' as delete_author,\n    count() AS c\nFROM git.clickhouse_line_changes\nWHERE (sign = -1) AND (file_extension IN ('h', 'cpp')) AND (line_type NOT IN ('Punct', 'Empty')) AND (author != prev_author) AND (prev_author != '')\nGROUP BY\n    prev_author,\n    author\nORDER BY c DESC\nLIMIT 1 BY prev_author\nLIMIT 100",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "authors-that-tends-to-rewrite-other-authors-code"
    },
    {
      "id": "WXPKFJCAHOKYKEVTWNFVCY",
      "name": "Top contributors per week day",
      "group": "git",
      "comment": "Identifies the top contributors for each day of the week based on commit frequency in the git.clickhouse_commits database.",
      "query": "SELECT\n    day_of_week,\n    author,\n    count() AS c\nFROM git.clickhouse_commits\nGROUP BY\n    dayOfWeek(time) AS day_of_week,\n    author\nORDER BY\n    day_of_week ASC,\n    c DESC\nLIMIT 1 BY day_of_week",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "top-contributors-per-week-day"
    },
    {
      "id": "8YRJGHFTNJAWJ96XCJKKEH",
      "name": "Top contributors per week day for last year",
      "group": "git",
      "comment": "Identifies the top commit contributor for each weekday over the past year by counting commits in the ClickHouse repository, sorted by day of week and number of contributions.",
      "query": "SELECT\n    day_of_week,\n    author,\n    count() AS c\nFROM git.clickhouse_commits\nWHERE time > (now() - toIntervalYear(1))\nGROUP BY\n    dayOfWeek(time) AS day_of_week,\n    author\nORDER BY\n    day_of_week ASC,\n    c DESC\nLIMIT 1 BY day_of_week",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "top-contributors-per-week-day-for-last-year"
    },
    {
      "id": "VQF4KMRDSUEXGS1JFVDJHV",
      "name": "Top contributor each day as a fraction of the total work performed in the last year",
      "group": "git",
      "comment": "Calculates the percentage of total work performed by the top author for each day of the week over the past year, based on lines of code added and deleted.",
      "query": "SELECT\n    top_author.day_of_week,\n    top_author.author,\n    top_author.author_work / all_work.total_work AS top_author_percent\nFROM\n(\n    SELECT\n        day_of_week,\n        author,\n        sum(lines_added) + sum(lines_deleted) AS author_work\n    FROM git.clickhouse_file_changes\n    WHERE time > (now() - toIntervalYear(1))\n    GROUP BY\n        author,\n        dayOfWeek(time) AS day_of_week\n    ORDER BY\n        day_of_week ASC,\n        author_work DESC\n    LIMIT 1 BY day_of_week\n) AS top_author\nINNER JOIN\n(\n    SELECT\n        day_of_week,\n        sum(lines_added) + sum(lines_deleted) AS total_work\n    FROM git.clickhouse_file_changes\n    WHERE time > (now() - toIntervalYear(1))\n    GROUP BY dayOfWeek(time) AS day_of_week\n) AS all_work USING (day_of_week)",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "top-contributor-each-day-as-a-fraction-of-the-total-work-performed-in-the-last-year"
    },
    {
      "id": "6YWAUQYPZINZDJGBEZBNWG",
      "name": "Distribution of code age across repository",
      "group": "git",
      "comment": "Analyzes and displays the average, minimum, and maximum age of files in terms of days across different folders in a repository while filtering out specific directories and file types.",
      "query": "WITH current_files AS\n    (\n        SELECT path\n        FROM\n        (\n            SELECT\n                old_path AS path,\n                max(time) AS last_time,\n                2 AS change_type\n            FROM git.clickhouse_file_changes\n            GROUP BY old_path\n            UNION ALL\n            SELECT\n                path,\n                max(time) AS last_time,\n                argMax(change_type, time) AS change_type\n            FROM git.clickhouse_file_changes\n            GROUP BY path\n        )\n        GROUP BY path\n        HAVING (argMax(change_type, last_time) != 2) AND (NOT match(path, '(^dbms/)|(^libs/)|(^tests/testflows/)|(^programs/server/store/)'))\n        ORDER BY path ASC\n    )\nSELECT\n    concat(root, '/', sub_folder) AS folder,\n    round(avg(days_present)) AS avg_age_of_files,\n    min(days_present) AS min_age_files,\n    max(days_present) AS max_age_files,\n    count() AS c\nFROM\n(\n    SELECT\n        path,\n        dateDiff('day', min(time), toDate('2022-11-03')) AS days_present\n    FROM git.clickhouse_file_changes\n    WHERE (path IN (current_files)) AND (file_extension IN ('h', 'cpp', 'sql'))\n    GROUP BY path\n)\nGROUP BY\n    splitByChar('/', path)[1] AS root,\n    splitByChar('/', path)[2] AS sub_folder\nORDER BY\n    root ASC,\n    c DESC\nLIMIT 5 BY root\n",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "distribution-of-code-age-across-repository"
    },
    {
      "id": "T4DTWTB36WFSEYAZLMGRNF",
      "name": "Number of an authors contribution divided by the number of lines they have had removed by another contributor",
      "group": "git",
      "comment": "Calculate the ratio of code lines written by an author to the lines removed by another contributor, filtering for file types 'h' and 'cpp' and excluding punctuations and empty lines, to identify authors with the highest remove ratio among those who have written over 1000 lines.",
      "query": "SELECT\n    k,\n    written_code.c,\n    removed_code.c,\n    removed_code.c / written_code.c AS remove_ratio\nFROM\n(\n    SELECT\n        author AS k,\n        count() AS c\n    FROM git.clickhouse_line_changes\n    WHERE (sign = 1) AND (file_extension IN ('h', 'cpp')) AND (line_type NOT IN ('Punct', 'Empty'))\n    GROUP BY k\n) AS written_code\nINNER JOIN\n(\n    SELECT\n        prev_author AS k,\n        count() AS c\n    FROM git.clickhouse_line_changes\n    WHERE (sign = -1) AND (file_extension IN ('h', 'cpp')) AND (line_type NOT IN ('Punct', 'Empty')) AND (author != prev_author)\n    GROUP BY k\n) AS removed_code USING (k)\nWHERE written_code.c > 1000\nORDER BY remove_ratio DESC\nLIMIT 10",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "number-of-an-authors-contribution-divided-by-the-number-of-lines-they-have-had-removed-by-another-contributor"
    },
    {
      "id": "5PL1QLNSH6QQTR8H9HINNP",
      "name": "Files with most rewrites",
      "group": "git",
      "comment": "Identifies the top 10 files with the highest number of significant rewrites, where more than 50% of their content was both added and removed during modifications, and their size exceeded 50 lines.",
      "query": "WITH\n    current_files AS\n    (\n        SELECT path\n        FROM\n        (\n            SELECT\n                old_path AS path,\n                max(time) AS last_time,\n                2 AS change_type\n            FROM git.clickhouse_file_changes\n            GROUP BY old_path\n            UNION ALL\n            SELECT\n                path,\n                max(time) AS last_time,\n                argMax(change_type, time) AS change_type\n            FROM git.clickhouse_file_changes\n            GROUP BY path\n        )\n        GROUP BY path\n        HAVING (argMax(change_type, last_time) != 2) AND (NOT match(path, '(^dbms/)|(^libs/)|(^tests/testflows/)|(^programs/server/store/)'))\n        ORDER BY path ASC\n    ),\n    changes AS\n    (\n        SELECT\n            path,\n            max(time) AS max_time,\n            commit_hash,\n            any(lines_added) AS num_added,\n            any(lines_deleted) AS num_deleted,\n            any(change_type) AS type\n        FROM git.clickhouse_file_changes\n        WHERE (change_type IN ('Add', 'Modify')) AND (path IN (current_files)) AND (file_extension IN ('h', 'cpp', 'sql'))\n        GROUP BY\n            path,\n            commit_hash\n        ORDER BY\n            path ASC,\n            max_time ASC\n    ),\n    rewrites AS\n    (\n        SELECT\n            path,\n            commit_hash,\n            max_time,\n            type,\n            num_added,\n            num_deleted,\n            sum(num_added - num_deleted) OVER (PARTITION BY path ORDER BY max_time ASC) AS current_size,\n            if(current_size > 0, num_added / current_size, 0) AS percent_add,\n            if(current_size > 0, num_deleted / current_size, 0) AS percent_delete\n        FROM changes\n    )\nSELECT\n    path,\n    count() AS num_rewrites\nFROM rewrites\nWHERE (type = 'Modify') AND (percent_add >= 0.5) AND (percent_delete >= 0.5) AND (current_size > 50)\nGROUP BY path\nORDER BY num_rewrites DESC\nLIMIT 10",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "files-with-most-rewrites"
    },
    {
      "id": "GVF23LEZTNZI22BT8LZBBE",
      "name": "Find weekday when the code have the highest chance to stay",
      "group": "git",
      "comment": "Analyzes which weekday newly added code is most likely to persist based on average presence days and occurrence frequency.",
      "query": "SELECT\n    day_of_week_added,\n    count() AS num,\n    avg(days_present) AS avg_days_present\nFROM\n(\n    SELECT\n        added_code.line,\n        added_code.time AS added_day,\n        dateDiff('day', added_code.time, removed_code.time) AS days_present\n    FROM\n    (\n        SELECT\n            path,\n            line,\n            max(time) AS time\n        FROM git.clickhouse_line_changes\n        WHERE (sign = 1) AND (line_type NOT IN ('Punct', 'Empty'))\n        GROUP BY\n            path,\n            line\n    ) AS added_code\n    INNER JOIN\n    (\n        SELECT\n            path,\n            line,\n            max(time) AS time\n        FROM git.clickhouse_line_changes\n        WHERE (sign = -1) AND (line_type NOT IN ('Punct', 'Empty'))\n        GROUP BY\n            path,\n            line\n    ) AS removed_code USING (path, line)\n    WHERE removed_code.time > added_code.time\n)\nGROUP BY dayOfWeek(added_day) AS day_of_week_added",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "find-weekday-when-the-code-have-the-highest-chance-to-stay"
    },
    {
      "id": "3CYYT7HEHWRFHVCM9JCKSU",
      "name": "Files sorted by average code age",
      "group": "git",
      "comment": "Lists the top 10 files with the highest average code age by calculating the average duration that lines of code persisted before being removed, while excluding specific directories and file types.",
      "query": "WITH\n    current_files AS\n    (\n        SELECT path\n        FROM\n        (\n            SELECT\n                old_path AS path,\n                max(time) AS last_time,\n                2 AS change_type\n            FROM git.clickhouse_file_changes\n            GROUP BY old_path\n            UNION ALL\n            SELECT\n                path,\n                max(time) AS last_time,\n                argMax(change_type, time) AS change_type\n            FROM git.clickhouse_file_changes\n            GROUP BY path\n        )\n        GROUP BY path\n        HAVING (argMax(change_type, last_time) != 2) AND (NOT match(path, '(^dbms/)|(^libs/)|(^tests/testflows/)|(^programs/server/store/)'))\n        ORDER BY path ASC\n    ),\n    lines_removed AS\n    (\n        SELECT\n            added_code.path AS path,\n            added_code.line,\n            added_code.time AS added_day,\n            dateDiff('day', added_code.time, removed_code.time) AS days_present\n        FROM\n        (\n            SELECT\n                path,\n                line,\n                max(time) AS time,\n                any(file_extension) AS file_extension\n            FROM git.clickhouse_line_changes\n            WHERE (sign = 1) AND (line_type NOT IN ('Punct', 'Empty'))\n            GROUP BY\n                path,\n                line\n        ) AS added_code\n        INNER JOIN\n        (\n            SELECT\n                path,\n                line,\n                max(time) AS time\n            FROM git.clickhouse_line_changes\n            WHERE (sign = -1) AND (line_type NOT IN ('Punct', 'Empty'))\n            GROUP BY\n                path,\n                line\n        ) AS removed_code USING (path, line)\n        WHERE (removed_code.time > added_code.time) AND (path IN (current_files)) AND (file_extension IN ('h', 'cpp', 'sql'))\n    )\nSELECT\n    path,\n    avg(days_present) AS avg_code_age\nFROM lines_removed\nGROUP BY path\nORDER BY avg_code_age DESC\nLIMIT 10",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "files-sorted-by-average-code-age"
    },
    {
      "id": "JGKZSEQDPDTDKZXD3ZCGLE",
      "name": "Authors who write more tests",
      "group": "git",
      "comment": "Identifies top 20 authors with significant code contributions, highlighting their focus on tests vs. code ratio.",
      "query": "SELECT\n    author,\n    countIf((file_extension IN ('h', 'cpp', 'sql', 'sh', 'py', 'expect')) AND (path LIKE '%tests%')) AS test,\n    countIf((file_extension IN ('h', 'cpp', 'sql')) AND (NOT (path LIKE '%tests%'))) AS code,\n    code / (code + test) AS ratio_code\nFROM git.clickhouse_file_changes\nGROUP BY author\nHAVING code > 20\nORDER BY code DESC\nLIMIT 20",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "authors-who-write-more-tests"
    },
    {
      "id": "S5AJIIRGSUAY1JXEVHQDAK",
      "name": "Plot distribution of authors who write more tests",
      "group": "geo",
      "comment": "Visualizes the distribution of authors based on the percentage of code written as tests out of total written code, using a histogram to highlight authors with significant test contributions.",
      "query": "WITH (\n        SELECT histogram(10)(ratio_code) AS hist\n        FROM\n        (\n            SELECT\n                author,\n                countIf((file_extension IN ('h', 'cpp', 'sql', 'sh', 'py', 'expect')) AND (path LIKE '%tests%')) AS test,\n                countIf((file_extension IN ('h', 'cpp', 'sql')) AND (NOT (path LIKE '%tests%'))) AS code,\n                code / (code + test) AS ratio_code\n            FROM git.clickhouse_file_changes\n            GROUP BY author\n            HAVING code > 20\n            ORDER BY code DESC\n            LIMIT 20\n        )\n    ) AS hist\nSELECT\n    arrayJoin(hist).1 AS lower,\n    arrayJoin(hist).2 AS upper,\n    bar(arrayJoin(hist).3, 0, 100, 500) AS bar",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "plot-distribution-of-authors-who-write-more-tests"
    },
    {
      "id": "EXPHDIURBTOXXOK1TGNNYD",
      "name": "Authors with most comments ratio",
      "group": "git",
      "comment": "Identifies the top 10 authors with the highest average comment-to-code ratio, ordered by total code contribution.",
      "query": "SELECT\n    author,\n    avg(ratio_comments) AS avg_ratio_comments,\n    sum(code) AS code\nFROM\n(\n    SELECT\n        author,\n        commit_hash,\n        countIf(line_type = 'Comment') AS comments,\n        countIf(line_type = 'Code') AS code,\n        if(comments > 0, comments / (comments + code), 0) AS ratio_comments\n    FROM git.clickhouse_line_changes\n    GROUP BY\n        author,\n        commit_hash\n)\nGROUP BY author\nORDER BY code DESC\nLIMIT 10",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "authors-with-most-comments-ratio"
    },
    {
      "id": "SBHEWR8XC4PRHY13HPPKCN",
      "name": "Average time before code will be rewritten ",
      "group": "git",
      "comment": "Calculates the average code-to-comment ratio over time, measured in weeks, for code contributions in C++ and SQL files, specifically focusing on code contributions exceeding 20 lines, and aggregates data every 10 weeks to analyze rewrite trends.",
      "query": "WITH author_ratios_by_offset AS\n    (\n        SELECT\n            author,\n            dateDiff('week', start_dates.start_date, contributions.week) AS week_offset,\n            ratio_code\n        FROM\n        (\n            SELECT\n                author,\n                toStartOfWeek(min(time)) AS start_date\n            FROM git.clickhouse_line_changes\n            WHERE file_extension IN ('h', 'cpp', 'sql')\n            GROUP BY author AS start_dates\n        ) AS start_dates\n        INNER JOIN\n        (\n            SELECT\n                author,\n                countIf(line_type = 'Code') AS code,\n                countIf((line_type = 'Comment') OR (line_type = 'Punct')) AS comments,\n                comments / (comments + code) AS ratio_code,\n                toStartOfWeek(time) AS week\n            FROM git.clickhouse_line_changes\n            WHERE (file_extension IN ('h', 'cpp', 'sql')) AND (sign = 1)\n            GROUP BY\n                time,\n                author\n            HAVING code > 20\n            ORDER BY\n                author ASC,\n                time ASC\n        ) AS contributions USING (author)\n    )\nSELECT\n    week_offset,\n    avg(ratio_code) AS avg_code_ratio\nFROM author_ratios_by_offset\nGROUP BY week_offset\nHAVING (week_offset % 10) = 0\nORDER BY week_offset ASC\nLIMIT 20\n",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "average-time-before-code-will-be-rewritten-"
    },
    {
      "id": "8PQNWEWHAJTGN6FTX59KH2",
      "name": "Day of the month with most number of rewrites",
      "group": "git",
      "comment": "Analyzes version control data to identify the day of the week with the highest number of significant file rewrites.",
      "query": "WITH\n    changes AS\n    (\n        SELECT\n            path,\n            commit_hash,\n            max_time,\n            type,\n            num_added,\n            num_deleted,\n            sum(num_added - num_deleted) OVER (PARTITION BY path ORDER BY max_time ASC) AS current_size,\n            if(current_size > 0, num_added / current_size, 0) AS percent_add,\n            if(current_size > 0, num_deleted / current_size, 0) AS percent_delete\n        FROM\n        (\n            SELECT\n                path,\n                max(time) AS max_time,\n                commit_hash,\n                any(file_lines_added) AS num_added,\n                any(file_lines_deleted) AS num_deleted,\n                any(file_change_type) AS type\n            FROM git.clickhouse_line_changes\n            WHERE (file_change_type IN ('Add', 'Modify')) AND (file_extension IN ('h', 'cpp', 'sql'))\n            GROUP BY\n                path,\n                commit_hash\n            ORDER BY\n                path ASC,\n                max_time ASC\n        )\n    ),\n    rewrites AS\n    (\n        SELECT any(max_time) OVER (PARTITION BY path ORDER BY max_time ASC ROWS BETWEEN 1 PRECEDING AND CURRENT ROW) AS previous_rewrite\n        FROM changes\n        WHERE (type = 'Modify') AND (percent_add >= 0.5) AND (percent_delete >= 0.5) AND (current_size > 50)\n    )\nSELECT\n    dayOfWeek(previous_rewrite) AS dayOfWeek,\n    count() AS num_re_writes\nFROM rewrites\nGROUP BY dayOfWeek",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "day-of-the-month-with-most-number-of-rewrites"
    },
    {
      "id": "BKHLVVWN5SET1VTIFQ8JVK",
      "name": "Authors with the most sticky code",
      "group": "git",
      "comment": "Identifies authors whose code changes, especially significant additions and deletions, demonstrate enduring relevance across multiple files by calculating average intervals between substantial modifications.",
      "query": "WITH\n    changes AS\n    (\n        SELECT\n            path,\n            author,\n            commit_hash,\n            max_time,\n            type,\n            num_added,\n            num_deleted,\n            sum(num_added - num_deleted) OVER (PARTITION BY path ORDER BY max_time ASC) AS current_size,\n            if(current_size > 0, num_added / current_size, 0) AS percent_add,\n            if(current_size > 0, num_deleted / current_size, 0) AS percent_delete\n        FROM\n        (\n            SELECT\n                path,\n                any(author) AS author,\n                max(time) AS max_time,\n                commit_hash,\n                any(file_lines_added) AS num_added,\n                any(file_lines_deleted) AS num_deleted,\n                any(file_change_type) AS type\n            FROM git.clickhouse_line_changes\n            WHERE (file_change_type IN ('Add', 'Modify')) AND (file_extension IN ('h', 'cpp', 'sql'))\n            GROUP BY\n                path,\n                commit_hash\n            ORDER BY\n                path ASC,\n                max_time ASC\n        )\n    ),\n    rewrites AS\n    (\n        SELECT\n            *,\n            any(max_time) OVER (PARTITION BY path ORDER BY max_time ASC ROWS BETWEEN 1 PRECEDING AND CURRENT ROW) AS previous_rewrite,\n            dateDiff('day', previous_rewrite, max_time) AS rewrite_days,\n            any(author) OVER (PARTITION BY path ORDER BY max_time ASC ROWS BETWEEN 1 PRECEDING AND CURRENT ROW) AS prev_author\n        FROM changes\n        WHERE (type = 'Modify') AND (percent_add >= 0.5) AND (percent_delete >= 0.5) AND (current_size > 50)\n    )\nSELECT\n    prev_author,\n    avg(rewrite_days) AS c,\n    uniq(path) AS num_files\nFROM rewrites\nGROUP BY prev_author\nHAVING num_files > 2\nORDER BY c DESC\nLIMIT 10\n",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "authors-with-the-most-sticky-code"
    },
    {
      "id": "S3E64UYCAMDAYJRSXINVFR",
      "name": "Most consecutive days of commits by an author",
      "group": "git",
      "comment": "Identifies the top authors with the longest streak of consecutive days of commits by calculating and ranking their continuous contribution days.",
      "query": "WITH commit_days AS\n    (\n        SELECT\n            author,\n            day,\n            any(day) OVER (PARTITION BY author ORDER BY day ASC ROWS BETWEEN 1 PRECEDING AND CURRENT ROW) AS previous_commit,\n            dateDiff('day', previous_commit, day) AS days_since_last,\n            if(days_since_last = 1, 1, 0) AS consecutive_day\n        FROM\n        (\n            SELECT\n                author,\n                toStartOfDay(time) AS day\n            FROM git.clickhouse_commits\n            GROUP BY\n                author,\n                day\n            ORDER BY\n                author ASC,\n                day ASC\n        )\n    )\nSELECT\n    author,\n    arrayMax(arrayMap(x -> length(x), arraySplit(x -> (x = 0), groupArray(consecutive_day)))) AS max_consecutive_days\nFROM commit_days\nGROUP BY author\nORDER BY max_consecutive_days DESC\nLIMIT 10",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "most-consecutive-days-of-commits-by-an-author"
    },
    {
      "id": "AKTW3Z8JZAPQ4H9BH2ZFRX",
      "name": "Line by line commit history of a file",
      "group": "git",
      "comment": "Fetches the commit history with rename changes for the file 'StorageReplicatedMergeTree.cpp' from the 'git' database table 'clickhouse_file_changes'.",
      "query": "SELECT\n    time,\n    path,\n    old_path,\n    commit_hash,\n    commit_message\nFROM git.clickhouse_file_changes\nWHERE (path = 'src/Storages/StorageReplicatedMergeTree.cpp') AND (change_type = 'Rename')",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "line-by-line-commit-history-of-a-file"
    },
    {
      "id": "KB5KQJJFNBKHE5GBUJCP1B",
      "name": "Menu with caviar",
      "group": "food",
      "comment": "Analyzes menu data from various years, filtering for items containing caviar and displaying trends in average prices using a line chart.",
      "query": "SELECT\n    round(toUInt32OrZero(extract(menu_date, '^\\\\d{4}')), -1) AS d,\n    count(),\n    round(avg(price), 2),\n    bar(avg(price), 0, 50, 100),\n    any(dish_name)\nFROM food.menu_item_denorm\nWHERE (menu_currency IN ('Dollars', '')) AND (d > 0) AND (d < 2022) AND (dish_name ILIKE '%caviar%')\nGROUP BY d\nORDER BY d ASC",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "menu-with-caviar"
    },
    {
      "id": "KBGB8WAATJ1TEZSHUTF197",
      "name": "Select ten rows",
      "group": "uk",
      "comment": "Fetches the first ten entries from the UK price paid dataset.",
      "query": "SELECT * FROM uk.uk_price_paid LIMIT 10    \n",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "select-ten-rows"
    },
    {
      "id": "TRCWH5ZETY4SEEK8ISCCAX",
      "name": "Most expensive district",
      "group": "uk",
      "comment": "Identifies and lists the top 100 UK towns and districts with the highest average property prices since 2020, provided at least 100 transactions occurred, ordered by descending price.",
      "query": "SELECT town, district, count() AS c, round(avg(price)) AS price, bar(price, 0, 5000000, 100) FROM uk.uk_price_paid WHERE date >= '2020-01-01' GROUP BY town, district HAVING c >= 100 ORDER BY price DESC LIMIT 100",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "most-expensive-district"
    },
    {
      "id": "EAI3T1FVZSGSRJFSSDCCQP",
      "name": "Average price per town",
      "group": "uk",
      "comment": "Calculates the average property price for each town from the UK price database.",
      "query": "SELECT \n    town,\n    avg(price) AS avg_price \nFROM uk.uk_price_paid\nGROUP BY town;",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "average-price-per-town"
    },
    {
      "id": "3CQY9DMYYK7PJSDRPGBJAE",
      "name": "Average price per year",
      "group": "uk",
      "comment": "Calculates the average price of flats per year, rounding the result and ordering by year in ascending sequence.",
      "query": "SELECT\n    toYear(date) AS year,\n    round(avg(price)) AS price\nFROM uk.uk_price_paid\nWHERE type = 'flat'\nGROUP BY year\nORDER BY year ASC",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "average-price-per-year"
    },
    {
      "id": "VX9XNLHPDAU9BROIOJNFXH",
      "name": "Average price per district of Bristol",
      "group": "uk",
      "comment": "Calculate the average housing price for each postcode in Bristol, listing the top 10 by descending order of price.",
      "query": "SELECT\n        postcode1,\n        round(avg(price)) AS price\nFROM uk.uk_price_paid\nWHERE (town = 'BRISTOL') AND (postcode1 != '')\nGROUP BY postcode1\nORDER BY price DESC\nLIMIT 10",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "average-price-per-district-of-bristol"
    },
    {
      "id": "EDRZIQMQN3FPZJEU3TPMKR",
      "name": "Median price change in London",
      "group": "uk",
      "comment": "Calculate the percentage change in median house prices from 2002 to 2022 across London postcodes, ordered by largest change.",
      "query": "SELECT\n        postcode1,\n        medianIf(price, toYear(date) = 2002) AS median_2002,\n        medianIf(price, toYear(date) = 2022) AS median_2022,\n        round(((median_2022 - median_2002) / median_2002) * 100) AS percent_change\nFROM uk.uk_price_paid\nWHERE town = 'LONDON'\nGROUP BY postcode1\nORDER BY percent_change DESC",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "median-price-change-in-london"
    },
    {
      "id": "7PEVSKK5MBGK5PTEQ6FUOD",
      "name": "10 years price change per district",
      "group": "uk",
      "comment": "Calculates the percentage change in median property prices per postcode district in London from 2002 to 2021.",
      "query": "SELECT\n        postcode1,\n        medianIf(price, toYear(date) = 2002) AS median_2002,\n        medianIf(price, toYear(date) = 2021) AS median_2021,\n        round(((median_2021 - median_2002) / median_2002) * 100) AS percent_change\nFROM uk.uk_price_paid\nWHERE town = 'LONDON'\nGROUP BY postcode1\nORDER BY percent_change ASC",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "10-years-price-change-per-district"
    },
    {
      "id": "6CZ8TRMMKWHYDE91A3KETE",
      "name": "Show create table",
      "group": "uk",
      "comment": "Displays the SQL statement used to create the 'uk_price_paid' table.",
      "query": "SHOW CREATE TABLE uk.uk_price_paid",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "show-create-table"
    },
    {
      "id": "TEH4J3REPN2FNDRGCYDNM6",
      "name": "Top 3 largest sale in London",
      "group": "uk",
      "comment": "Identifies the top three highest property sale prices in London by county.",
      "query": "SELECT\n    county,\n    price\nFROM uk.uk_price_paid\nWHERE town = 'LONDON'\nORDER BY price DESC\nLIMIT 3",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "top-3-largest-sale-in-london"
    },
    {
      "id": "DXP1BNTN6YUGWXRZJET4AM",
      "name": "Top 3 largest sale in London - optimized primary keys",
      "group": "uk",
      "comment": "Displays the top three highest property sale prices in London by county in descending order.",
      "query": "SELECT\n    county,\n    price\nFROM uk.uk_price_paid_oby_town_price\nWHERE town = 'LONDON'\nORDER BY price DESC\nLIMIT 3\n",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "top-3-largest-sale-in-london-optimized-primary-keys"
    },
    {
      "id": "5INADD1EKJOHLERC8VMGAP",
      "name": "Explain 1 - Top 3 largest sale in London - optimized primary keys",
      "group": "uk",
      "comment": "Identifies the top 3 most expensive property sales in London, sorted by price in descending order, to analyze using optimized primary keys.",
      "query": "EXPLAIN actions = 1\nSELECT\n    county,\n    price\nFROM uk.uk_price_paid_oby_town_price\nWHERE town = 'LONDON'\nORDER BY price DESC\nLIMIT 3",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "explain-1-top-3-largest-sale-in-london-optimized-primary-keys"
    },
    {
      "id": "6EG4PWKERYSYU3BB6BMSDX",
      "name": "Explain Pipeline - Top 3 largest sale in London - optimized primary keys",
      "group": "uk",
      "comment": "Analyzes the query execution plan for retrieving the top three highest property sale prices in London, optimized by primary keys.",
      "query": "EXPLAIN PIPELINE\nSELECT\n    county,\n    price\nFROM uk.uk_price_paid\nWHERE town = 'LONDON'\nORDER BY price DESC\nLIMIT 3",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "explain-pipeline-top-3-largest-sale-in-london-optimized-primary-keys"
    },
    {
      "id": "AEZD2XA3HDMMAJDYTJCZLD",
      "name": "Top 3 most expensive counties",
      "group": "uk",
      "comment": "Identifies the top three counties in the UK with the highest average property prices.",
      "query": "SELECT\n    county,\n    avg(price)\nFROM uk.uk_price_paid\nGROUP BY county\nORDER BY avg(price) DESC\nLIMIT 3\n",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "top-3-most-expensive-counties"
    },
    {
      "id": "6IDMHK3OMR1C97J6M9EUQS",
      "name": "Select one row",
      "group": "uk",
      "comment": "Fetches a single row of data from the 'uk_price_paid' table.",
      "query": "SELECT * FROM uk.uk_price_paid LIMIT 1",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "select-one-row"
    },
    {
      "id": "HDNRS1WAM3BEZKOEUWRAF3",
      "name": "Average price for one district",
      "group": "uk",
      "comment": "With optimize_aggregation_in_order=1, the query would be able to shortcut and as a result process less data.",
      "query": "SELECT\n    postcode1, postcode2,\n    formatReadableQuantity(avg(price)) AS avg_price\nFROM uk.uk_price_paid\nGROUP BY postcode1, postcode2\nLIMIT 1;",
      "chart": {
        "type": "line",
        "config": {
          "xaxis": "day",
          "yaxis": "change"
        }
      },
      "format": false,
      "params": [],
      "slug": "average-price-for-one-district"
    },
    {
      "id": "5DVCXJZPH77BQJKREMEG8Z",
      "name": "Average house price and sales per month since 1995",
      "group": "uk",
      "comment": "Analyzes monthly sales count and average prices for leasehold and freehold properties in the UK starting from 1995.",
      "query": "SELECT\n    month,\n    countIf(duration = 'leasehold') AS `Leasehold Sold`,\n    countIf(duration = 'freehold') AS `Freehold Sold`,\n    avgIf(price, duration = 'freehold') AS `Average Freehold Price`,\n    avgIf(price, duration = 'leasehold') AS `Average Leasehold Price`\nFROM uk.uk_price_paid\nGROUP BY toStartOfMonth(date) AS month\nORDER BY month ASC",
      "chart": {
        "type": "line",
        "config": {
          "xaxis": "day",
          "yaxis": "change"
        }
      },
      "format": false,
      "params": [],
      "slug": "average-house-price-and-sales-per-month-since-1995"
    },
    {
      "id": "W2VWYX9FLEK9MPXFEVM9UR",
      "name": "Regions with the largest percentage change in median house prices in the last 20 years",
      "group": "uk",
      "comment": "Identifies UK regions with the highest percentage increase in median house prices from 2000 to 2020 by calculating the change between these years and ranking them in descending order.",
      "query": "SELECT\n   code,\n   (anyIf(med_2020, med_2020 > 0) - anyIf(med_2000, med_2000 > 0)) / anyIf(med_2000, med_2000 > 0) AS percent_change\nFROM\n(\n   SELECT\n       code,\n       medianIf(price, year = 2000) AS med_2000,\n       medianIf(price, year = 2020) AS med_2020\n   FROM\n   (\n       SELECT\n           date,\n           price,\n           locality,\n           town,\n           district,\n           county,\n           code\n       FROM uk.uk_price_paid\n       LEFT JOIN uk.uk_codes AS codes ON (uk.uk_price_paid.county = codes.name) OR (uk.uk_price_paid.district = codes.name) OR (uk.uk_price_paid.town = codes.name) OR (uk.uk_price_paid.locality = codes.name) OR (replaceAll(uk.uk_price_paid.district, 'CITY OF ', '') = codes.name)\n   )\n   WHERE (code != '') AND ((toYear(date) = 2000) OR (toYear(date) = 2020))\n   GROUP BY\n       code,\n       toYear(date) AS year\n   ORDER BY code ASC\n)\nGROUP BY code\nORDER BY percent_change DESC",
      "chart": {
        "type": "bar",
        "config": {
          "xaxis": "year",
          "yaxis": "avg_precipitation",
          "series": "country",
          "stack": true
        }
      },
      "format": false,
      "params": [],
      "slug": "regions-with-the-largest-percentage-change-in-median-house-prices-in-the-last-20-years"
    },
    {
      "id": "M4FSVBVMSHY98NKCQP8N4K",
      "name": "Ontime flight per day of the week",
      "group": "ontime",
      "comment": "Counts the number of on-time flights for each day of the week from 2000 to 2008, ordered by the number of flights in descending order.",
      "query": "SELECT DayOfWeek, count(*) AS c\nFROM ontime.ontime\nWHERE Year>=2000 AND Year<=2008\nGROUP BY DayOfWeek\nORDER BY c DESC;\n",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "ontime-flight-per-day-of-the-week"
    },
    {
      "id": "BIPDVQNIGVEZFQYFEFQB7O",
      "name": "Airport with most departures",
      "group": "opensky",
      "comment": "Identifies the airport with the highest number of departures, calculates the average distance of these departures, and represents the distance distribution visually.",
      "query": "SELECT\n    origin,\n    count(),\n    round(avg(geoDistance(longitude_1, latitude_1, longitude_2, latitude_2))) AS distance,\n    bar(distance, 0, 10000000, 100) AS bar\nFROM opensky.opensky\nWHERE origin != ''\nGROUP BY origin\nORDER BY count() DESC\nLIMIT 100",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "airport-with-most-departures"
    },
    {
      "id": "9IUASIZZPK6JYG3BU4RPUM",
      "name": "Year start avg price per town",
      "group": "uk",
      "comment": "Calculates the yearly starting average property price for the top 10 towns (excluding 'GATWICK') with highest average prices, ordered by year.",
      "query": "SELECT\n    toStartOfYear(date) AS time,\n    town,\n    round(avg(price)) AS price\nFROM uk.uk_price_paid\nWHERE town IN (\n    SELECT town\n    FROM uk.uk_price_paid\n    WHERE town != 'GATWICK'\n    GROUP BY town\n    ORDER BY avg(price) DESC\n    LIMIT 10\n)\nGROUP BY\n    time,\n    town\nORDER BY time ASC",
      "chart": {
        "type": "area",
        "config": {
          "xaxis": "time",
          "yaxis": "price",
          "series": "town"
        }
      },
      "format": false,
      "params": [],
      "slug": "year-start-avg-price-per-town"
    },
    {
      "id": "F2EBMWNXBFRG722XG2MGGV",
      "name": "Year start avg price for a manual list of towns",
      "group": "uk",
      "comment": "Calculates the average starting year house price for specified high-demand districts in and around London and other selected UK towns, displaying data in an ascending time series.",
      "query": "\nSELECT\n    toStartOfYear(date) AS time,\n    district,\n    round(avg(price)) AS price\nFROM uk.uk_price_paid\nWHERE (district IN (\n    SELECT district\n    FROM uk.uk_price_paid\n    WHERE town = 'LONDON'\n    GROUP BY district\n    ORDER BY avg(price) DESC\n    LIMIT 10\n)) AND (district IN ('TOWER HAMLETS', 'HACKNEY', 'NEWHAM', 'CITY OF LONDON', 'WALTHAM FOREST', 'REDBRIDGE', 'BARKING AND DAGENHAM', 'HAVERING', 'HARINGEY', 'EPPING FOREST', 'ISLINGTON', 'CAMDEN', 'CITY OF WESTMINSTER', 'BARNET', 'HARROW', 'HILLINGDON', 'ENFIELD', 'EALING', 'HOUNSLOW', 'HAMMERSMITH AND FULHAM', 'LEWISHAM', 'BRENT', 'WANDSWORTH', 'SOUTHWARK', 'LAMBETH', 'GREENWICH', 'KENSINGTON AND CHELSEA', 'MERTON', 'BROMLEY', 'RICHMOND UPON THAMES', 'CROYDON', 'BEXLEY', 'KINGSTON UPON THAMES', 'HARLOW', 'SUTTON', 'CITY OF BRISTOL', 'MALVERN HILLS', 'THURROCK', 'RHONDDA CYNON TAFF'))\nGROUP BY\n    time,\n    district\nORDER BY time ASC",
      "chart": {
        "type": "area",
        "config": {
          "xaxis": "time",
          "yaxis": "price",
          "series": "district"
        }
      },
      "format": false,
      "params": [],
      "slug": "year-start-avg-price-for-a-manual-list-of-towns"
    },
    {
      "id": "8VHKVFS4NZFQGTZNKOWTH1",
      "name": "Year start avg price for all towns with simple condition",
      "group": "uk",
      "comment": "Calculates the average starting year price for the top 10 districts by price in London.",
      "query": "SELECT\n    toStartOfYear(date) AS time,\n    district,\n    round(avg(price)) AS price\nFROM uk.uk_price_paid\nWHERE (district IN (\n    SELECT district\n    FROM uk.uk_price_paid\n    WHERE uk.uk_price_paid.town = 'LONDON'\n    GROUP BY district\n    ORDER BY avg(price) DESC\n    LIMIT 10\n)) AND (1 = 1)\nGROUP BY\n    time,\n    district\nORDER BY time ASC",
      "chart": {
        "type": "area",
        "config": {
          "xaxis": "time",
          "yaxis": "price",
          "series": "district"
        }
      },
      "format": false,
      "params": [],
      "slug": "year-start-avg-price-for-all-towns-with-simple-condition"
    },
    {
      "id": "MH8X8UBKWAEPZJD7JHA1RH",
      "name": "Count all rows",
      "group": "noaa",
      "comment": "Counts the total number of rows in the NOAA database.",
      "query": "SELECT count() FROM noaa.noaa;",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "count-all-rows"
    },
    {
      "id": "GIKBNUA8S14RNEJ2SXU4IJ",
      "name": "Highest temperature in Portugal",
      "group": "noaa",
      "comment": "Extracts the top 5 recorded maximum temperatures in Portugal from NOAA data, presenting the values, dates, and locations of these records.",
      "query": "SELECT\n    tempMax / 10 AS maxTemp,\n    station_id,\n    date,\n    location\nFROM noaa.noaa\nWHERE substring(station_id, 1, 2) = 'PO'\nORDER BY tempMax DESC\nLIMIT 5",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "highest-temperature-in-portugal"
    },
    {
      "id": "61NDTT5XYXTZEBTVKMN5W2",
      "name": "Highest temperature in Portugal using subquery",
      "group": "noaa",
      "comment": "Finds the top 5 highest recorded temperatures in Portugal by utilizing a subquery to filter data from relevant weather stations.",
      "query": "SELECT\n    tempMax / 10 AS maxTemp,\n    station_id,\n    date,\n    location,\n    name\nFROM noaa.noaa\nWHERE station_id IN (\n    SELECT station_id\n    FROM noaa.stations\n    WHERE country_code = 'PO'\n)\nORDER BY tempMax DESC\nLIMIT 5",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "highest-temperature-in-portugal-using-subquery"
    },
    {
      "id": "W7QQFHSZVARIBM8FLCJFJF",
      "name": "Highest temperature in Portugal using Joins",
      "group": "noaa",
      "comment": "This query is not optimized and can not be executed due to memory limit quota",
      "query": "SELECT\n    tempMax / 10 AS maxTemp,\n    station_id,\n    date,\n    stations.name AS name,\n    (stations.lat, stations.lon) AS location\nFROM noaa.noaa\nINNER JOIN noaa.stations ON noaa.station_id = stations.station_id\nWHERE stations.country_code = 'PO'\nORDER BY tempMax DESC\nLIMIT 5\n",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "highest-temperature-in-portugal-using-joins"
    },
    {
      "id": "CSFKIYZ88AJGRHVNXRYYI3",
      "name": "Select one entry from dict",
      "group": "noaa",
      "comment": "Retrieves the 'state' field for the specific station ID 'CA00116HFF6' from the NOAA stations dictionary.",
      "query": "SELECT dictGet(noaa.stations_dict, 'state', 'CA00116HFF6')",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "select-one-entry-from-dict"
    },
    {
      "id": "2APN6W2QFMGF4EVYIMWCC3",
      "name": "Highest temperature in Portugal using dictionary unoptimized",
      "group": "noaa",
      "comment": "Finds the top 5 maximum temperatures recorded in Portugal using station dictionaries for location data.",
      "query": "SELECT\n    tempMax / 10 AS maxTemp,\n    station_id,\n    date,\n    (dictGet('noaa.stations_dict', 'lat', station_id), dictGet('noaa.stations_dict', 'lon', station_id)) AS location,\n    dictGet('noaa.stations_dict', 'name', station_id) AS name\nFROM noaa.noaa\nWHERE station_id IN (\n    SELECT station_id\n    FROM noaa.stations\n    WHERE country_code = 'PO'\n)\nORDER BY tempMax DESC\nLIMIT 5",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "highest-temperature-in-portugal-using-dictionary-unoptimized"
    },
    {
      "id": "NJM1DPH69PXZYW6XA5TUKX",
      "name": "Highest temperature in Portugal using dictionary optimized",
      "group": "noaa",
      "comment": "Identifies the top 5 highest maximum temperatures recorded at weather stations in Portugal, utilizing dictionary functions for efficiency.",
      "query": "SELECT\n    tempMax / 10 AS maxTemp,\n    station_id,\n    date,\n    (dictGet('noaa.stations_dict', 'lat', station_id), dictGet('noaa.stations_dict', 'lon', station_id)) AS location,\n    dictGet('noaa.stations_dict', 'name', station_id) AS name\nFROM noaa.noaa\nWHERE dictGet('noaa.stations_dict', 'country_code', station_id) = 'PO'\nORDER BY tempMax DESC\nLIMIT 5",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "highest-temperature-in-portugal-using-dictionary-optimized"
    },
    {
      "id": "T7DVXL4IRQSYDZXUZEMYOL",
      "name": "Weather events in the United States",
      "group": "noaa",
      "comment": "Provides the two highest weekly wind speeds and average precipitation in the U.S. for low elevation areas, filtered between June and October, highlighting significant weather events.",
      "query": "SELECT\n    week,\n    toYear(week) AS year,\n    lat, lon,\n    avg_precipitation,\n    max_wind_speed * 10\nFROM\n(\n    SELECT\n        geoHash,\n        week,\n        geohashDecode(geoHash) AS lonlat,\n        lonlat.1 AS lon,\n        lonlat.2 AS lat,\n        max(maxWindSpeed) AS max_wind_speed,\n        avg(precipitation)/10 AS avg_precipitation\n    FROM noaa.noaa\n    WHERE (dictGet(country.country_polygons, 'name', location) IN ('United States of America')) AND (elevation < 500) AND toMonth(date) BETWEEN 6 AND 10\n    GROUP BY\n        geohashEncode(location.1, location.2, 4) AS geoHash,\n        toStartOfWeek(date) AS week\n    HAVING max_wind_speed > 300  AND avg_precipitation > 20      \n    ORDER BY\n        max_wind_speed DESC\n)\nORDER BY\n    year ASC,\n    max_wind_speed DESC\nLIMIT 2 BY year",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "weather-events-in-the-united-states"
    },
    {
      "id": "HKEUQL4GMCRQCMTLKDFM6I",
      "name": "Get two states",
      "group": "noaa",
      "comment": "Retrieves two records from the states data table in the NOAA database without any specific filtering.",
      "query": "SELECT *\nFROM noaa.states\nLIMIT 2",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "get-two-states"
    },
    {
      "id": "GQVMWG2TB9VOFJPBRXCRPZ",
      "name": "Top ski resorts by amount of snow",
      "group": "noaa",
      "comment": "Ranks top ski resorts within 20 km of weather stations based on highest recorded snowfall since 2017, shown in meters, for U.S. locations above 1800 meters elevation.",
      "query": "SELECT\n    resort_name,\n    total_snow / 1000 AS total_snow_m,\n    resort_location,\n    month_year\nFROM\n(\n    SELECT\n        resort_name,\n        highest_snow.station_id,\n        geoDistance(lon, lat, station_location.1, station_location.2) / 1000 AS distance_km,\n        highest_snow.total_snow,\n        station_location,\n        month_year,\n        (lon, lat) AS resort_location\n    FROM\n    (\n        SELECT\n            sum(snowfall) AS total_snow,\n            station_id,\n            any(location) AS station_location,\n            month_year,\n            substring(station_id, 1, 2) AS code\n        FROM noaa.noaa\n        WHERE (date > '2017-01-01') AND (code = 'US') AND (elevation > 1800)\n        GROUP BY\n            station_id,\n            toYYYYMM(date) AS month_year\n        ORDER BY total_snow DESC\n        LIMIT 1000\n    ) AS highest_snow\n    INNER JOIN noaa.resorts ON highest_snow.code = resorts.code\n    WHERE distance_km < 20\n    ORDER BY\n        resort_name ASC,\n        total_snow DESC\n    LIMIT 1 BY\n        resort_name,\n        station_id\n)\nORDER BY total_snow DESC\nLIMIT 5",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "top-ski-resorts-by-amount-of-snow"
    },
    {
      "id": "JTSQ6ZMB8QTXWBFRKPD3J2",
      "name": "Top ski resorts by amount of snow using dictionary",
      "group": "noaa",
      "comment": "Ranks top ski resorts based on snowfall, filtered by proximity to NOAA stations within 20 km, and considers resorts above 1800m elevation since 2017.",
      "query": "SELECT\n    resort_name,\n    total_snow / 1000 AS total_snow_m,\n    resort_location,\n    month_year\nFROM\n(\n    SELECT\n        resort_name,\n        highest_snow.station_id,\n        geoDistance(resorts_dict.lon, resorts_dict.lat, station_lon, station_lat) / 1000 AS distance_km,\n        highest_snow.total_snow,\n        (resorts_dict.lon, resorts_dict.lat) as resort_location,\n        month_year\n    FROM\n    (\n        SELECT\n            sum(snowfall) AS total_snow,\n            station_id,\n            dictGet('noaa.stations_dict', 'lat', station_id) AS station_lat,\n            dictGet('noaa.stations_dict', 'lon', station_id) AS station_lon,\n            month_year,\n            dictGet('noaa.stations_dict', 'state', station_id) AS state\n        FROM noaa.noaa\n        WHERE (date > '2017-01-01') AND (state != '') AND (elevation > 1800)\n        GROUP BY\n            station_id,\n            toYYYYMM(date) AS month_year\n        ORDER BY total_snow DESC\n        LIMIT 1000\n    ) AS highest_snow\n    INNER JOIN noaa.resorts_dict ON highest_snow.state = resorts_dict.state\n    WHERE distance_km < 20\n    ORDER BY\n        resort_name ASC,\n        total_snow DESC\n    LIMIT 1 BY resort_name, station_id\n)\nORDER BY total_snow DESC\nLIMIT 5",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "top-ski-resorts-by-amount-of-snow-using-dictionary"
    },
    {
      "id": "HNQECUKC2F4GGRKHXWRTGS",
      "name": "World's coldest countries",
      "group": "noaa",
      "comment": "Identifies the coldest countries by calculating the lowest recorded temperatures since 1970 from weather station data.",
      "query": "SELECT\n    code,\n    min(tempMin) / 10 AS min_temp\nFROM noaa.noaa\nWHERE date > '1970-01-01'\nGROUP BY substring(station_id, 1, 2) AS code\nLIMIT 1000",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "worlds-coldest-countries"
    },
    {
      "id": "DN1SAY1IIIEAWRPF4JHFCG",
      "name": "Find areas with best holidays conditions",
      "group": "noaa",
      "comment": "Identifies geographic areas with optimal holiday conditions by analyzing average sunlight, temperature, precipitation, and elevation within specific climate criteria.",
      "query": "SELECT\n    geoHash,\n    month,\n    avg(percentDailySun) AS avg_daily_sun,\n    geohashDecode(geoHash) AS lonlat,\n    lonlat.1 AS lon,\n    lonlat.2 AS lat,\n    avg(tempAvg) / 10 AS avg_temp,\n    max(tempMax) / 10 AS max_temp,\n    min(tempMin) / 10 AS min_temp,\n    sum(precipitation) AS sum_precipitation,\n    avg(elevation) AS avg_elevation\nFROM noaa.noaa\nWHERE date > '1970-01-01'\nGROUP BY\n    geohashEncode(location.1, location.2, 4) AS geoHash,\n    toMonth(date) AS month\nHAVING (max_temp < 40) AND (min_temp > 0) AND (avg_temp > 10) AND (sum_precipitation < 100) AND (avg_elevation < 1000)",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "find-areas-with-best-holidays-conditions"
    },
    {
      "id": "9MFVCCCQDPMXAB6TMG6LQZ",
      "name": "Group locations per temperature conditions",
      "group": "noaa",
      "comment": "Classifies and counts geographical locations based on various temperature and precipitation conditions, including criteria such as 'not too cold' and 'ideal temperature with minimal rain'.",
      "query": "SELECT\n    values.1 AS labels,\n    values.2 AS count\nFROM\n(\n    SELECT arrayJoin([('not_too_cold', countIf(min_temp > 0)), ('not_too_cold_or_cold', countIf((min_temp > 0) AND (max_temp < 40))), ('ideal_temp', countIf((max_temp < 40) AND (min_temp > 0) AND (avg_temp > 10))), ('ideal_temp_min_rain', countIf((max_temp < 40) AND (min_temp > 0) AND (avg_temp > 10) AND (sum_precipitation < 100))), ('ideal_temp_min_rain_not_high', countIf((max_temp < 40) AND (min_temp > 0) AND (avg_temp > 10) AND (sum_precipitation < 100) AND (avg_elevation < 1000)))]) AS values\n    FROM\n    (\n        SELECT\n            geoHash,\n            month,\n            avg(percentDailySun) AS avg_daily_sun,\n            geohashDecode(geoHash) AS lonlat,\n            lonlat.1 AS lat,\n            lonlat.2 AS lon,\n            avg(tempAvg) / 10 AS avg_temp,\n            max(tempMax) / 10 AS max_temp,\n            min(tempMin) / 10 AS min_temp,\n            sum(precipitation) AS sum_precipitation,\n            avg(elevation) AS avg_elevation\n        FROM noaa.noaa\n        WHERE date > '1970-01-01'\n        GROUP BY\n            geohashEncode(location.1, location.2, 4) AS geoHash,\n            toMonth(date) AS month\n    )\n)",
      "chart": {
        "type": "line",
        "config": {
          "xaxis": "day",
          "yaxis": "change"
        }
      },
      "format": false,
      "params": [],
      "slug": "group-locations-per-temperature-conditions"
    },
    {
      "id": "9IZCFAXDJXUUZTFM1E2H6S",
      "name": "Columns size - Codec V1",
      "group": "noaa",
      "comment": "Displays the compressed and uncompressed size of each column in the 'noaa_codec_v1' table along with their compression ratio.",
      "query": "SELECT\n    name,\n    formatReadableSize(sum(data_compressed_bytes)) AS compressed_size,\n    formatReadableSize(sum(data_uncompressed_bytes)) AS uncompressed_size,\n    round(sum(data_uncompressed_bytes) / sum(data_compressed_bytes), 2) AS ratio\nFROM system.columns\nWHERE table = 'noaa_codec_v1'\nGROUP BY name\nORDER BY sum(data_compressed_bytes) DESC",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "columns-size-codec-v1"
    },
    {
      "id": "HKYQD4RDKKTMPGJ4JDOCTU",
      "name": "Columns size - Codec V2",
      "group": "noaa",
      "comment": "Calculates and compares the compressed and uncompressed data sizes for each column in the 'noaa_codec_v2' table, and provides the compression ratio, ordered by the largest compressed size.",
      "query": "SELECT\n    name,\n    formatReadableSize(sum(data_compressed_bytes)) AS compressed_size,\n    formatReadableSize(sum(data_uncompressed_bytes)) AS uncompressed_size,\n    round(sum(data_uncompressed_bytes) / sum(data_compressed_bytes), 2) AS ratio\nFROM system.columns\nWHERE table = 'noaa_codec_v2'\nGROUP BY name\nORDER BY sum(data_compressed_bytes) DESC",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "columns-size-codec-v2"
    },
    {
      "id": "MBYL39M5R5OXUB6OHKKC7Q",
      "name": "Columns size - Codec V3",
      "group": "noaa",
      "comment": "Analyzes the column sizes and compression ratio for the 'noaa_codec_v3' table by calculating the readable compressed and uncompressed data sizes and sorts them by compressed data size.",
      "query": "SELECT\n    name,\n    formatReadableSize(sum(data_compressed_bytes)) AS compressed_size,\n    formatReadableSize(sum(data_uncompressed_bytes)) AS uncompressed_size,\n    round(sum(data_uncompressed_bytes) / sum(data_compressed_bytes), 2) AS ratio\nFROM system.columns\nWHERE table = 'noaa_codec_v3'\nGROUP BY name\nORDER BY sum(data_compressed_bytes) DESC",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "columns-size-codec-v3"
    },
    {
      "id": "DSWEF4PNB6GSGXVHGQB3OS",
      "name": "Columns size - Codec V4",
      "group": "noaa",
      "comment": "Analyzes column sizes for 'noaa_codec_v4' table, displaying compressed and uncompressed data sizes along with compression ratios.",
      "query": "SELECT\n    name,\n    formatReadableSize(sum(data_compressed_bytes)) AS compressed_size,\n    formatReadableSize(sum(data_uncompressed_bytes)) AS uncompressed_size,\n    round(sum(data_uncompressed_bytes) / sum(data_compressed_bytes), 2) AS ratio\nFROM system.columns\nWHERE table = 'noaa_codec_v4'\nGROUP BY name\nORDER BY sum(data_compressed_bytes) DESC",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "columns-size-codec-v4"
    },
    {
      "id": "QJSXJHRRSDPNHK7OQKHNDJ",
      "name": "Columns size - Codec optimal",
      "group": "noaa",
      "comment": "Returns the readable sizes and compression ratios of columns in the 'noaa_codec_optimal' table, sorted by compressed byte size.",
      "query": "SELECT\n    name,\n    formatReadableSize(sum(data_compressed_bytes)) AS compressed_size,\n    formatReadableSize(sum(data_uncompressed_bytes)) AS uncompressed_size,\n    round(sum(data_uncompressed_bytes) / sum(data_compressed_bytes), 2) AS ratio\nFROM system.columns\nWHERE table = 'noaa_codec_optimal'\nGROUP BY name\nORDER BY sum(data_compressed_bytes) DESC",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "columns-size-codec-optimal"
    },
    {
      "id": "BUXPNOJD2BMPNLR4KXSGMB",
      "name": "Most effective codec per column",
      "group": "noaa",
      "comment": "Determines the most effective compression codec per column for tables matching 'noaa_codec%' in terms of smallest compressed size.",
      "query": "SELECT\n    name,\n    if(argMin(compression_codec, data_compressed_bytes) != '', argMin(compression_codec, data_compressed_bytes), 'DEFAULT') AS best_codec,\n    formatReadableSize(min(data_compressed_bytes)) AS compressed_size\nFROM system.columns\nWHERE table LIKE 'noaa_codec%'\nGROUP BY name",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "most-effective-codec-per-column"
    },
    {
      "id": "4V8I2PE4TVXENP7JUUJTZ9",
      "name": "Table size - Codec V1",
      "group": "noaa",
      "comment": "Calculates the total compressed and uncompressed table sizes and their compression ratio for the 'noaa_codec_v1' table in human-readable format.",
      "query": "SELECT\n    formatReadableSize(sum(data_compressed_bytes)) AS compressed_size,\n    formatReadableSize(sum(data_uncompressed_bytes)) AS uncompressed_size,\n    round(sum(data_uncompressed_bytes) / sum(data_compressed_bytes), 2) AS ratio\nFROM system.columns\nWHERE table = 'noaa_codec_v1'",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "table-size-codec-v1"
    },
    {
      "id": "RUFWKLMGR4VGVBTTDLY4IQ",
      "name": "Table size - Codec V2",
      "group": "noaa",
      "comment": "Returns the readable compressed size, uncompressed size, and compression ratio for the \"noaa_codec_v2\" table.",
      "query": "SELECT\n    formatReadableSize(sum(data_compressed_bytes)) AS compressed_size,\n    formatReadableSize(sum(data_uncompressed_bytes)) AS uncompressed_size,\n    round(sum(data_uncompressed_bytes) / sum(data_compressed_bytes), 2) AS ratio\nFROM system.columns\nWHERE table = 'noaa_codec_v2'",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "table-size-codec-v2"
    },
    {
      "id": "UVYOSRZVJDNLKZVVKDWBGM",
      "name": "Table size - Codec V3",
      "group": "noaa",
      "comment": "Calculate the total compressed and uncompressed sizes of the 'noaa_codec_v3' table and determine the compression ratio.",
      "query": "SELECT\n    formatReadableSize(sum(data_compressed_bytes)) AS compressed_size,\n    formatReadableSize(sum(data_uncompressed_bytes)) AS uncompressed_size,\n    round(sum(data_uncompressed_bytes) / sum(data_compressed_bytes), 2) AS ratio\nFROM system.columns\nWHERE table = 'noaa_codec_v3'",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "table-size-codec-v3"
    },
    {
      "id": "7US7RPFC3PNFZSDEAYKBYJ",
      "name": "Table size - Codec V4",
      "group": "noaa",
      "comment": "Calculates and compares the compressed and uncompressed data size for 'noaa_codec_v4' table and derives the compression ratio.",
      "query": "SELECT\n    formatReadableSize(sum(data_compressed_bytes)) AS compressed_size,\n    formatReadableSize(sum(data_uncompressed_bytes)) AS uncompressed_size,\n    sum(data_uncompressed_bytes) / sum(data_compressed_bytes) AS compression_ratio\nFROM system.columns\nWHERE table = 'noaa_codec_v4'",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "table-size-codec-v4"
    },
    {
      "id": "X9OZKPJT7GFLZBCSDGYYN6",
      "name": "Table size - Codec optimal",
      "group": "noaa",
      "comment": "Calculates and displays the total compressed and uncompressed sizes of data in a specific NOAA table, along with the compression ratio.",
      "query": "SELECT\n    formatReadableSize(sum(data_compressed_bytes)) AS compressed_size,\n    formatReadableSize(sum(data_uncompressed_bytes)) AS uncompressed_size,\n    round(sum(data_uncompressed_bytes) / sum(data_compressed_bytes), 2) AS ratio\nFROM system.columns\nWHERE table = 'noaa_codec_optimal'",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "table-size-codec-optimal"
    },
    {
      "id": "DRO8ZSB69FH6LFQWA7AS4Z",
      "name": "Weather statistics for elevations every 100m - codec V1",
      "group": "noaa",
      "comment": "Analyzes weather statistics by grouping and ordering data by elevation range in increments of 100 meters to provide insights into temperature extremes, precipitation totals, average sunshine, wind speed, and snowfall for US stations since 1970.",
      "query": "SELECT\n    elevation_range,\n    uniq(station_id) AS num_stations,\n    max(tempMax) / 10 AS max_temp,\n    min(tempMin) / 10 AS min_temp,\n    sum(precipitation) AS total_precipitation,\n    avg(percentDailySun) AS avg_percent_sunshine,\n    max(maxWindSpeed) AS max_wind_speed,\n    sum(snowfall) AS total_snowfall\nFROM noaa.noaa_codec_v1\nWHERE (date > '1970-01-01') AND (station_id IN (\n    SELECT station_id\n    FROM noaa.stations\n    WHERE country_code = 'US'\n))\nGROUP BY floor(elevation, -2) AS elevation_range\nORDER BY elevation_range ASC",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "weather-statistics-for-elevations-every-100m-codec-v1"
    },
    {
      "id": "7XY7YJF2KWSX5VRZHGSD9U",
      "name": "Min/Max value boundaries",
      "group": "noaa",
      "comment": "Extracts the minimum and maximum values for wind, temperature, snow, and precipitation from the NOAA dataset.",
      "query": "SELECT\n    COLUMNS('Wind|temp|snow|pre') APPLY min,\n    COLUMNS('Wind|temp|snow|pre') APPLY max\nFROM noaa.noaa",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "minmax-value-boundaries"
    },
    {
      "id": "EDQSLZ32CKBUWT1QQJ6TWM",
      "name": "Average precipitation per year by country",
      "group": "noaa",
      "comment": "Calculates the average annual precipitation by country using data from the NOAA after 1970, excluding specified country codes, and displays the results in a bar chart.",
      "query": "SELECT year,\n       avg(`precipitation`) AS `avg_precipitation`,\n       dictGet(`country`.`country_iso_codes`, 'name', code) as country\nFROM `noaa`.`noaa_v2`\nWHERE date > '1970-01-01' AND code IN ('AL', 'AN', 'AU', 'BE', 'BO', 'CY', 'DA', 'EI', 'EZ', 'EN', 'FI', 'FR', 'GG', 'GI', 'GK', 'GM',\n'GR', 'HR', 'HU', 'IC', 'IM', 'IT', 'JE', 'LG', 'LH', 'LO', 'LS', 'LU', 'MD', 'MK', 'MN', 'MT', 'NL', 'NO', 'PL', 'PO', 'RO', 'SI', 'SM',\n'SP', 'SW', 'SZ', 'TU', 'UK', 'UP', 'VT')\nGROUP BY toStartOfYear(`date`) AS `year`,\n         substring(station_id, 1, 2) as code\nHAVING avg_precipitation > 0         \nORDER BY country, year ASC\nLIMIT 100000",
      "chart": {
        "type": "bar",
        "config": {
          "xaxis": "year",
          "yaxis": "avg_precipitation",
          "series": "country",
          "stack": true
        }
      },
      "format": false,
      "params": [],
      "slug": "average-precipitation-per-year-by-country"
    },
    {
      "id": "HZZD59S222CCIUI7MVW8AR",
      "name": "Show tables in noaa",
      "group": "noaa",
      "comment": "Lists the tables within the NOAA database that have the name 'noaa'.",
      "query": "SHOW TABLES in noaa WHERE name = 'noaa'    ",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "show-tables-in-noaa"
    },
    {
      "id": "4RNIAAXZVWK2YLFFVGEC1O",
      "name": "Number of station with no precipitation measured",
      "group": "noaa",
      "comment": "Calculates the number and ratio of weather stations with zero precipitation measurements compared to those with non-zero measurements.",
      "query": "SELECT\n    countIf(precipitation = 0) AS num_empty,\n    countIf(precipitation > 0) AS num_non_zero,\n    num_empty / (num_empty + num_non_zero) AS ratio\nFROM noaa.noaa",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "number-of-station-with-no-precipitation-measured"
    },
    {
      "id": "XZH9SXO4NNJ5OQ1PITDN5C",
      "name": "Weather statistics for elevations every 100m - codec V3",
      "group": "noaa",
      "comment": "Aggregates and analyzes weather data metrics by 100m elevation ranges in the US, including temperature extremes, precipitation, sunshine, wind speed, and snowfall.",
      "query": "SELECT\n    elevation_range,\n    uniq(station_id) AS num_stations,\n    max(tempMax) / 10 AS max_temp,\n    min(tempMin) / 10 AS min_temp,\n    sum(precipitation) AS total_precipitation,\n    avg(percentDailySun) AS avg_percent_sunshine,\n    max(maxWindSpeed) AS max_wind_speed,\n    sum(snowfall) AS total_snowfall\nFROM noaa.noaa_codec_v3\nWHERE (date > '1970-01-01') AND (station_id IN (\n    SELECT station_id\n    FROM noaa.stations\n    WHERE country_code = 'US'\n))\nGROUP BY floor(elevation, -2) AS elevation_range\nORDER BY elevation_range ASC",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "weather-statistics-for-elevations-every-100m-codec-v3"
    },
    {
      "id": "OWCAGSZEFJVH5F6DAHAHVJ",
      "name": "Weather statistics for elevations every 100m - codec optimal",
      "group": "noaa",
      "comment": "Aggregates weather data for US stations grouped by 100m elevation intervals to analyze maximum and minimum temperatures, total precipitation, average sunshine percentage, maximum wind speed, and total snowfall since 1970.",
      "query": "SELECT\n    elevation_range,\n    uniq(station_id) AS num_stations,\n    max(tempMax) / 10 AS max_temp,\n    min(tempMin) / 10 AS min_temp,\n    sum(precipitation) AS total_precipitation,\n    avg(percentDailySun) AS avg_percent_sunshine,\n    max(maxWindSpeed) AS max_wind_speed,\n    sum(snowfall) AS total_snowfall\nFROM noaa.noaa_codec_optimal\nWHERE (date > '1970-01-01') AND (station_id IN (\n    SELECT station_id\n    FROM noaa.stations\n    WHERE country_code = 'US'\n))\nGROUP BY floor(elevation, -2) AS elevation_range\nORDER BY elevation_range ASC",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "weather-statistics-for-elevations-every-100m-codec-optimal"
    },
    {
      "id": "GF7GFDIAUKNTGQOFBWLPF6",
      "name": "Highest temperature measured in the world",
      "group": "noaa",
      "comment": "Finds and lists the five highest recorded temperatures worldwide, including location and date, sorted by temperature and then by date.",
      "query": "SELECT\n    tempMax / 10 AS maxTemp,\n    location,\n    name,\n    date\nFROM noaa.noaa\nORDER BY\n    tempMax DESC,\n    date ASC\nLIMIT 5",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "highest-temperature-measured-in-the-world"
    },
    {
      "id": "JZRNS4M2OVRHBAKDVQNEUQ",
      "name": "Highest temperature measured in each Portuguese region.",
      "group": "noaa",
      "comment": "Queries the highest recorded temperatures from NOAA data for Portuguese regions, ordered by temperature in descending order, limited to the top five results.",
      "query": "SELECT\n    tempMax / 10 AS maxTemp,\n    station_id,\n    date,\n    location\nFROM noaa.noaa\nWHERE dictGet(country.country_polygons, 'name', location) = 'Portugal'\nORDER BY tempMax DESC\nLIMIT 5",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "highest-temperature-measured-in-each-portuguese-region"
    },
    {
      "id": "5XPYMUKF7RKCEFCUJWGDPE",
      "name": "Highest temperature measured in each Canadian region",
      "group": "noaa",
      "comment": "Identifies the highest recorded temperatures in Canadian regions, highlighting top five instances with associated details.",
      "query": "SELECT\n   tempMax / 10 AS maxTemp,\n   station_id,\n   date,\n   location\nFROM noaa.noaa\nWHERE dictGet(country.country_polygons, 'name', location) = 'Canada'\nORDER BY tempMax DESC\nLIMIT 5",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "highest-temperature-measured-in-each-canadian-region"
    },
    {
      "id": "29YQBREOAFP3QZNIARGTQO",
      "name": "Highest temperature measured in Canadian region matching a specific land area",
      "group": "noaa",
      "comment": "Retrieve the top 5 highest maximum temperatures recorded in Canadian stations.",
      "query": "SELECT\n   tempMax / 10 AS maxTemp,\n   station_id,\n   date,\n   location\nFROM noaa.noaa\nWHERE substring(station_id, 1, 2) = 'CA'\nORDER BY tempMax DESC\nLIMIT 5",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "highest-temperature-measured-in-canadian-region-matching-a-specific-land-area"
    },
    {
      "id": "NHQIA2R29W9QCV8QL9EPNA",
      "name": "Hottest areas of the United States and Mexico",
      "group": "noaa",
      "comment": "Analyzes temperature data to identify the areas in the United States and Mexico with the highest maximum temperatures since 1970, grouped by geohashes.",
      "query": "SELECT geoHash, geohashDecode(geoHash) as lon_lat, max(tempMax)/10 as max_temp\nFROM noaa.noaa\nWHERE date > '1970-01-01' and dictGet(country.country_polygons, 'name', location) IN ('United States of America', 'Mexico')\nGROUP BY geohashEncode(location.1, location.2, 3) as geoHash",
      "chart": {
        "type": "line",
        "config": {
          "xaxis": "day",
          "yaxis": "high"
        }
      },
      "format": false,
      "params": [],
      "slug": "hottest-areas-of-the-united-states-and-mexico"
    },
    {
      "id": "6B6MXXER1XG8J2QVTY3NMA",
      "name": "Forex table size",
      "group": "forex",
      "comment": "Calculates and displays the total compressed and uncompressed size of the 'forex' table, as well as the compression ratio.",
      "query": "SELECT\n    table,\n    formatReadableSize(sum(data_compressed_bytes)) AS compressed_size,\n    formatReadableSize(sum(data_uncompressed_bytes)) AS uncompressed_size,\n    sum(data_compressed_bytes) / sum(data_uncompressed_bytes) AS compression_ratio\nFROM system.columns\nWHERE (database = 'forex') AND (table = 'forex')\nGROUP BY table\nORDER BY table ASC",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "forex-table-size"
    },
    {
      "id": "BXNF4KPUROV9QRCS5FAGMD",
      "name": "Analysis of the GBP/EUR",
      "group": "forex",
      "comment": "Extracts and analyzes daily closing price changes in the forex pairing of GBP/EUR over the year 2016, presenting the data in a line chart format.",
      "query": "SELECT\n    base,\n    quote,\n    day,\n    close,\n    close - any(close) OVER (PARTITION BY base, quote ORDER BY base ASC, quote ASC, day ASC ROWS BETWEEN 1 PRECEDING AND CURRENT ROW) AS change\nFROM\n(\n    SELECT\n        base,\n        quote,\n        day,\n        argMax(ask, datetime) AS close\n    FROM forex.forex\n    WHERE (quote = 'GBP') AND (base = 'EUR') AND (datetime > '2016-01-01 00:00:00.000') AND (datetime < '2017-01-01 00:00:00.000')\n    GROUP BY\n        base,\n        quote,\n        toStartOfDay(datetime) AS day\n    ORDER BY\n        base ASC,\n        quote ASC,\n        day ASC\n)\nORDER BY\n    base ASC,\n    quote ASC,\n    day ASC",
      "chart": {
        "type": "line",
        "config": {
          "xaxis": "day",
          "yaxis": "change"
        }
      },
      "format": false,
      "params": [],
      "slug": "analysis-of-the-gbpeur"
    },
    {
      "id": "NUJZCPYKAMVRNRMWCXZTSY",
      "name": "Get first timestamp for each quotes",
      "group": "forex",
      "comment": "Determines the earliest timestamp for each currency pair in the forex_usd dataset.",
      "query": "SELECT\n    base,\n    quote,\n    min(datetime)\nFROM forex.forex_usd\nGROUP BY\n    base,\n    quote",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "get-first-timestamp-for-each-quotes"
    },
    {
      "id": "RKVF4YKRGPSSNGJ3DACN79",
      "name": "Periods of high spread in the EUR/USD",
      "group": "forex",
      "comment": "Identifies the top five days with the highest spread increase in EUR/USD by calculating the daily average spread change.",
      "query": "SELECT\n    base,\n    quote,\n    day,\n    spread - any(spread) OVER (PARTITION BY base, quote ORDER BY base ASC, quote ASC, day ASC ROWS BETWEEN 1 PRECEDING AND CURRENT ROW) AS change\nFROM\n(\n    SELECT\n        base,\n        quote,\n        avg(ask - bid) AS spread,\n        day\n    FROM forex.forex\n    WHERE (base = 'EUR') AND (quote = 'USD')\n    GROUP BY\n        base,\n        quote,\n        toYYYYMMDD(datetime) AS day\n    ORDER BY\n        base ASC,\n        quote ASC,\n        day ASC\n)\nORDER BY change DESC\nLIMIT 5\n",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "periods-of-high-spread-in-the-eurusd"
    },
    {
      "id": "FELR1JPYR2UGMNEUC1MTRN",
      "name": "Periods of high change in pairs involving the GBP",
      "group": "forex",
      "comment": "Identifies days with the highest change in currency pairs involving GBP since 2016.",
      "query": "WITH daily_change AS\n    (\n        SELECT\n            base,\n            quote,\n            day,\n            close,\n            close - any(close) OVER (PARTITION BY base, quote ORDER BY base ASC, quote ASC, day ASC ROWS BETWEEN 1 PRECEDING AND CURRENT ROW) AS change\n        FROM\n        (\n            SELECT\n                base,\n                quote,\n                day,\n                argMax(ask, datetime) AS close\n            FROM forex.forex\n            WHERE (quote = 'GBP') OR (base = 'GBP')\n            GROUP BY\n                base,\n                quote,\n                toStartOfDay(datetime) AS day\n            ORDER BY\n                base ASC,\n                quote ASC,\n                day ASC\n        )\n        ORDER BY\n            base ASC,\n            quote ASC,\n            day ASC\n    )\nSELECT *\nFROM daily_change\nWHERE day > '2016-01-02 00:00:00'\nORDER BY abs(change) DESC\nLIMIT 1 BY\n    base,\n    quote",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "periods-of-high-change-in-pairs-involving-the-gbp"
    },
    {
      "id": "QEB835S62CZDPVTOVPBUH9",
      "name": "USD price against the price of oil",
      "group": "forex",
      "comment": "Extracts daily maximum USD price against oil for financial analysis since 2010.",
      "query": "SELECT\n    day,\n    argMax(ask, datetime) AS price\nFROM forex.forex\nWHERE (datetime > '2010-01-01 00:00:00') AND (base = 'BCO') AND (quote = 'USD')\nGROUP BY toStartOfDay(datetime) AS day\nORDER BY day ASC",
      "chart": {
        "type": "line",
        "config": {
          "xaxis": "day",
          "yaxis": "price"
        }
      },
      "format": false,
      "params": [],
      "slug": "usd-price-against-the-price-of-oil"
    },
    {
      "id": "VHWZWZ4XWZTFP167AU9JAG",
      "name": "Volatility for a currency pair",
      "group": "forex",
      "comment": "Calculates volatility for GBP/USD by determining standard deviation of price changes over a 30-day window, and flags days as volatile if changes exceed this volatility.",
      "query": "SELECT\n    day,\n    stddevPop(change) OVER (PARTITION BY base, quote ORDER BY day ASC ROWS BETWEEN 30 PRECEDING AND CURRENT ROW) AS volatility,\n    if(abs(change) > volatility, 'true', 'false') AS volitile\nFROM\n(\n    SELECT\n        base,\n        quote,\n        day,\n        close,\n        close - any(close) OVER (PARTITION BY base, quote ORDER BY base ASC, quote ASC, day ASC ROWS BETWEEN 1 PRECEDING AND CURRENT ROW) AS change\n    FROM\n    (\n        SELECT\n            base,\n            quote,\n            day,\n            argMax(ask, datetime) AS close\n        FROM forex.forex\n        WHERE (quote = 'USD') AND (base = 'GBP') AND (datetime > '2010-01-01 00:00:00')\n        GROUP BY\n            base,\n            quote,\n            toStartOfDay(datetime) AS day\n        ORDER BY\n            base ASC,\n            quote ASC,\n            day ASC\n    )\n    ORDER BY\n        base ASC,\n        quote ASC,\n        day ASC\n)\nORDER BY\n    base ASC,\n    quote ASC,\n    day ASC",
      "chart": {
        "type": "line",
        "config": {
          "xaxis": "day",
          "yaxis": "volatility"
        }
      },
      "format": false,
      "params": [],
      "slug": "volatility-for-a-currency-pair"
    },
    {
      "id": "G5QCEZE3VY4CDVFE7MSIAE",
      "name": "EUR price evolution compared to other pairs",
      "group": "forex",
      "comment": "Analyzes daily closing bid prices for EUR against GBP, USD, NZD, and CND from mid-2016 to late-2021.",
      "query": "SELECT toStartOfDay(datetime) as time, quote, argMax(bid,datetime) as close\nFROM forex.forex\nWHERE (base ='EUR') AND quote IN ('GBP', 'USD', 'NZD', 'CND') AND datetime >= '1464739200' AND datetime <= '1633046400'\nGROUP BY time, quote\nORDER BY time ASC, quote ASC",
      "chart": {
        "type": "line",
        "config": {
          "xaxis": "time",
          "yaxis": "close",
          "series": "quote"
        }
      },
      "format": false,
      "params": [],
      "slug": "eur-price-evolution-compared-to-other-pairs"
    },
    {
      "id": "NJPUOAHZYRPELRF6DY2XFT",
      "name": "EUR price evolution against USD",
      "group": "forex",
      "comment": "Analyzes daily price evolution of EUR against USD, showing open, close, high, and low rates from June 2016 to September 2021.",
      "query": "SELECT toStartOfDay(datetime) as day, argMin(ask, datetime) as open, argMax(ask,datetime) as close, max(ask) as high, min(ask) as low\nFROM forex.forex\nWHERE (base ='EUR') AND quote ='USD' AND datetime >= '1464739200' AND datetime <= '1633046400'\nGROUP BY day\nORDER BY day ASC",
      "chart": {
        "type": "line",
        "config": {
          "xaxis": "day",
          "yaxis": "high"
        }
      },
      "format": false,
      "params": [],
      "slug": "eur-price-evolution-against-usd"
    },
    {
      "id": "VWNMT1VWGJACNVNNPZYMFD",
      "name": "All posts mentioning ClickHouse over time",
      "group": "hackernews",
      "comment": "Visualizes the frequency of posts mentioning ClickHouse on Hacker News by month and year as a line chart.",
      "query": "SELECT\n    toYYYYMM(toDateTime(time)) AS monthYear,\n    bar(count(), 0, 120, 20) AS count\nFROM hackernews.hackernews\nWHERE (text ILIKE '%ClickHouse%')\nGROUP BY monthYear\nORDER BY monthYear ASC",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "all-posts-mentioning-clickhouse-over-time"
    },
    {
      "id": "JYYNJM7UWYX5RIWU7GPGTJ",
      "name": "Post per month histogram",
      "group": "hackernews",
      "comment": "Displays a histogram of the number of Hacker News posts mentioning 'ClickHouse' per month.",
      "query": "SELECT count() as `posts per month`, toStartOfMonth(time) as month FROM hackernews.hackernews WHERE type IN ('comment', 'story') AND (text ILIKE '%ClickHouse%' OR title ILIKE '%ClickHouse%') GROUP BY month ORDER BY month LIMIT 1000",
      "chart": {
        "type": "bar",
        "config": {
          "xaxis": "month",
          "yaxis": "posts per month",
          "stack": false
        }
      },
      "format": false,
      "params": [],
      "slug": "post-per-month-histogram"
    },
    {
      "id": "PW9DCFW8AWGRHYHHQ7TODQ",
      "name": "Top phrases",
      "group": "hackernews",
      "comment": "Analyzes Hacker News data to identify and count the most frequent two-word phrases, excluding common stopwords, and displays the top 20 results in a horizontal bar chart.",
      "query": "WITH stop_words AS\n   (\n       SELECT token\n       FROM words.stop_words\n   )\nSELECT\n   phrase,\n   count() AS c\nFROM\n(\n   SELECT\n       arrayJoin(shingles) AS shingle,\n       concat(shingle.1, ' ', shingle.2) AS phrase\n   FROM\n   (\n       SELECT\n           tokens,\n           arrayFilter(t -> (NOT ((t.2) IS NULL)), arrayZip(tokens, arrayPushBack(arrayPopFront(tokens), NULL))) AS shingles\n       FROM\n       (\n           SELECT arrayFilter(t -> ((t NOT IN (stop_words)) AND (length(t) > 2)), alphaTokens(title)) AS tokens\n           FROM hackernews.hackernews\n           WHERE (type IN ('story', 'comment'))\n       )\n       \n       WHERE length(tokens) > 0\n   )\n)\nGROUP BY phrase\nORDER BY c DESC\nLIMIT 20",
      "chart": {
        "type": "horizontal bar",
        "config": {
          "xaxis": "phrase",
          "yaxis": "c"
        }
      },
      "format": false,
      "params": [],
      "slug": "top-phrases"
    },
    {
      "id": "24AZ83BWSK7DYB24TDJHZD",
      "name": "Logs histogram per level",
      "group": "logs",
      "comment": "Generates a bar chart histogram of logs filtered by severity levels (critical, error, warning, info) and ordered by timestamp, using data from HTTP logs.",
      "query": "SELECT (now() - (toDateTime('1998-05-08 13:44:46') - timestamp)) as log_time,\n       multiIf(message.status > 500, 'critical', message.status > 400, 'error', message.status > 300, 'warning',\n               'info')                                           as level,\n       message.request.method                                    as method,\n       message.status                                            as status,\n       message.size                                              as size,\n       message.request                                           as log\nFROM logs.http_logs\nORDER BY timestamp DESC\nLIMIT 10000",
      "chart": {
        "type": "bar",
        "config": {
          "xaxis": "log_time",
          "yaxis": "status",
          "series": "level",
          "stack": true
        }
      },
      "format": false,
      "params": [],
      "slug": "logs-histogram-per-level"
    },
    {
      "id": "PT6BDKJPNWWYVOSFHJ2N9H",
      "name": "Show logs sample",
      "group": "logs",
      "comment": "Displays a sample of 10 entries from the HTTP logs.",
      "query": "SELECT * FROM logs.http_logs LIMIT 10    \n",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "show-logs-sample"
    },
    {
      "id": "3ZBX6XXPDY7TNOP7QOWHAB",
      "name": "Show parity",
      "group": "numbers",
      "comment": "Displays each number and its corresponding parity as a string for the first five natural numbers.",
      "query": "SELECT\n    number,\n    parity_str(number)\nFROM numbers(5)",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "show-parity"
    },
    {
      "id": "SBUUEETHAQ5F2G22N83N5O",
      "name": "SELECT randCanonical",
      "group": "random",
      "comment": "Generates a random floating-point number between 0 (inclusive) and 1 (exclusive) using the randCanonical function.",
      "query": "SELECT randCanonical()",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "select-randcanonical"
    },
    {
      "id": "4NBCMXTYTA9YTCCZLQVXAX",
      "name": "SELECT randUniform",
      "group": "random",
      "comment": "Generates random uniform numbers within the range of 5 to 10.",
      "query": "SELECT randUniform(5,10)",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "select-randuniform"
    },
    {
      "id": "J6DVNJWSXSQS5GRDTRMPLH",
      "name": "SELECT randUniform",
      "group": "random",
      "comment": "Generates a random integer between 5 and 10 using randUniform and displays it as 'r'.\n",
      "query": "SELECT floor(randUniform(5, 10)) AS r\n",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "select-randuniform"
    },
    {
      "id": "QK3XFPOY1ENJWTGI3OV6W2",
      "name": "SELECT randNormal",
      "group": "random",
      "comment": "Generates a dataset of random numbers with a normal distribution, where the mean is 100 and the standard deviation is 5, and visualizes it in a line chart.",
      "query": "SELECT randNormal(100, 5)",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "select-randnormal"
    },
    {
      "id": "BZU8EGF4FDZWQ7WJYHEVG8",
      "name": "Visualize randNormal",
      "group": "random",
      "comment": "Generates a histogram of randomly generated normal distribution values from 0 to 100,000, displaying the frequency of each occurrence in a line chart.",
      "query": "SELECT\n    floor(randNormal(100, 5)) AS k,\n    count(*) AS c,\n    bar(c, 0, 50000, 100)\nFROM numbers(100000) GROUP BY k ORDER BY k ASC",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "visualize-randnormal"
    },
    {
      "id": "WPE9ESMN2DUMV1VXDNYFGG",
      "name": "Visualize randBinomial",
      "group": "random",
      "comment": "Generates a histogram of binomial distribution values computed from 100 trials with a success probability of 0.85.",
      "query": "SELECT\n    floor(randBinomial(100, 0.85)) AS k,\n    bar(count(*), 0, 50000, 100) AS b1\nFROM numbers(100000)\nGROUP BY k\nORDER BY k ASC",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "visualize-randbinomial"
    },
    {
      "id": "F22WJVPWARLKMGDIV1MSX3",
      "name": "Visualize randNegativeBinomial",
      "group": "random",
      "comment": "Generates a dataset using the negative binomial distribution and visualizes its frequency distribution with a line chart by grouping and ordering the random values.",
      "query": "SELECT\n    floor(randNegativeBinomial(100, 0.85)) AS k,\n    bar(count(*), 0, 50000, 100) AS b1\nFROM numbers(100000)\nGROUP BY k\nORDER BY k ASC",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "visualize-randnegativebinomial"
    },
    {
      "id": "89WD1BCBQH3CDS1PJW2RST",
      "name": "Visualize randLogNormal",
      "group": "random",
      "comment": "Visualizes a log-normal distribution by grouping and ordering random values, displaying the result as a line chart.",
      "query": "SELECT\n    floor(randLogNormal(1 / 100, 0.75)) AS k,\n    bar(count(*), 0, 50000, 10) AS b1\nFROM numbers(100000)\nGROUP BY k\nORDER BY k ASC",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "visualize-randlognormal"
    },
    {
      "id": "JZFQ4QXNR496JRBSYWQZFX",
      "name": "Visualize randExponential",
      "group": "random",
      "comment": "Generates a distribution of exponentially distributed random numbers, grouping and visualizing the frequency of each number using a line chart.",
      "query": "SELECT\n    floor(randExponential(1 / 2)) AS k,\n    bar(count(*), 0, 50000, 10) AS b1\nFROM numbers(100000)\nGROUP BY k\nORDER BY k ASC",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "visualize-randexponential"
    },
    {
      "id": "68A57FHXKATFL3BPRZSXPX",
      "name": "Visualize randChiSquared",
      "group": "random",
      "comment": "Generates a line chart to visualize the distribution of a chi-squared random variable with 10 degrees of freedom.",
      "query": "SELECT\n    floor(randChiSquared(10)) AS k,\n    bar(count(*), 0, 10000, 10) AS b1\nFROM numbers(100000)\nGROUP BY k\nORDER BY k ASC",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "visualize-randchisquared"
    },
    {
      "id": "RXS6OCJLLIW8F3YAMHYECY",
      "name": "Visualize randStudentT",
      "group": "random",
      "comment": "Visualizes the distribution of the Student's t random variable with 4.5 degrees of freedom by grouping and ordering the frequency of occurrences of integer values.",
      "query": "SELECT\n    floor(randStudentT(4.5)) AS k,\n    bar(count(*), 0, 10000, 10) AS b1\nFROM numbers(100000)\nGROUP BY k\nORDER BY k ASC",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "visualize-randstudentt"
    },
    {
      "id": "W68QKVQ8SRT4QCJMVK3X6T",
      "name": "Visualize randFisherF",
      "group": "random",
      "comment": "Generates a line chart visualizing the distribution of random values from a Fisher's F-distribution using the randFisherF function, grouped and ordered by floored values of the distribution.",
      "query": "SELECT\n    floor(randFisherF(3, 20)) AS k,\n    bar(count(*), 0, 10000, 10) AS b1\nFROM numbers(100000)\nGROUP BY k\nORDER BY k ASC",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "visualize-randfisherf"
    },
    {
      "id": "FABP4BRNHNFQGE9JJBWZNF",
      "name": "Visualize randPoisson",
      "group": "random",
      "comment": "Generates a line chart to visualize the distribution of a Poisson random variable simulated with a mean of 10 over 100,000 samples.",
      "query": "SELECT\n    floor(randPoisson(10)) AS k,\n    bar(count(*), 0, 15000, 10) AS b1\nFROM numbers(100000)\nGROUP BY k\nORDER BY k ASC",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "visualize-randpoisson"
    },
    {
      "id": "UUUNYD3J4X9MU3R1HZSFPG",
      "name": "Visualize randBernoulli",
      "group": "random",
      "comment": "Visualize the distribution of Bernoulli trials with probability 0.75 by counting occurrences over 100,000 trials and ordering results.",
      "query": "SELECT\n    floor(randBernoulli(0.75)) AS k,\n    count(*)\nFROM numbers(100000)\nGROUP BY k\nORDER BY k ASC",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "visualize-randbernoulli"
    },
    {
      "id": "TGPZRLB1RMKOUS6ZTFQHRT",
      "name": "Total spent distribution",
      "group": "random",
      "comment": "Visualizes the distribution of total spending in a dataset by grouping and counting purchases according to rounded down expenditure values.",
      "query": "SELECT\n    floor(total_spent) AS s,\n    count(*) AS n,\n    bar(n, 0, 350000, 50)\nFROM random.purchases\nGROUP BY s\nORDER BY s ASC",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "total-spent-distribution"
    },
    {
      "id": "1UU1ECB2IZWLCGULVAJMJM",
      "name": "Click events distribution",
      "group": "random",
      "comment": "Visualizes hourly distribution of click events with counts and bar representation up to 15000 events.",
      "query": "SELECT\n    toStartOfHour(dt) AS hour,\n    count(*) AS c,\n    bar(c, 0, 15000, 50)\nFROM random.events\nGROUP BY hour\nORDER BY hour ASC",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "click-events-distribution"
    },
    {
      "id": "JMDKIGQJRSLZYLTAVKAWNA",
      "name": "CPU load hourly distribution ",
      "group": "random",
      "comment": "Analyzes average CPU load per hour and visualizes data with a line chart for clearer interpretation.",
      "query": "SELECT\n    toStartOfHour(dt) AS h,\n    round(avg(val), 2) AS v,\n    bar(v, 0, 100)\nFROM random.metrics\nGROUP BY h\nORDER BY h ASC",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "cpu-load-hourly-distribution-"
    },
    {
      "id": "EKHAVPH67P9XQHJPFQWVHO",
      "name": "Select randBernoulli",
      "group": "random",
      "comment": "Generates a random boolean value following a Bernoulli distribution with a success probability of 0.9.",
      "query": "SELECT randBernoulli(0.9)",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "select-randbernoulli"
    },
    {
      "id": "J1A8SOEHBGZVTLSRKJVB7W",
      "name": "Simulate binary state",
      "group": "random",
      "comment": "Simulates a binary outcome with a 95% probability for \"success\" over 1000 trials and counts occurrences of each outcome.",
      "query": "SELECT\n    If(randBernoulli(0.95), 'success', 'failure') AS status,\n    count(*) AS c\nFROM numbers(1000)\nGROUP BY status",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "simulate-binary-state"
    },
    {
      "id": "FPTERBSNQFQ21TDQGTMAAR",
      "name": "Random HTTP codes",
      "group": "random",
      "comment": "Simulates and counts occurrences of random HTTP status codes from a sample of 1000 numbers.",
      "query": "SELECT\n    ['200', '404', '502', '403'][toInt32(randBinomial(4, 0.1)) + 1] AS http_code,\n    count(*)\nFROM numbers(1000)\nGROUP BY http_code\n",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "random-http-codes"
    },
    {
      "id": "BCIAZ32CS95NFRP3XZ3ZPM",
      "name": "Random strings",
      "group": "random",
      "comment": "Generates a list of random printable ASCII strings with varying lengths between 5 and 25 characters, and returns each string alongside its length.",
      "query": "SELECT\n    randomPrintableASCII(randUniform(5, 25)) AS s,\n    length(s) as length    \nFROM numbers(10)",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "random-strings"
    },
    {
      "id": "X7G6ZEFF4LVDKCZBH5GPMD",
      "name": "Generate noisy data",
      "group": "random",
      "comment": "Add random noise to a string to simulate data errors for testing purposes.",
      "query": "SELECT fuzzBits('Good string', 0.01)\nFROM numbers(10)\n",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "generate-noisy-data"
    },
    {
      "id": "4YGUG8FWTWZELB7DRG2GQY",
      "name": "Detect generated faulty strings",
      "group": "random",
      "comment": "Identifies if any alteration occurs when applying small fuzzing changes to a string and counts occurrences over multiple iterations.",
      "query": "SELECT\n    IF(fuzzBits('Good string', 0.001) = 'Good string', 1, 0) AS has_errors,\n    count(*)\nFROM numbers(1000)\nGROUP BY has_errors",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "detect-generated-faulty-strings"
    },
    {
      "id": "3H7CREOQQNO6TXGRPGB3QN",
      "name": "RandNormal distribution of clicks",
      "group": "random",
      "comment": "Aggregates and visualizes the distribution of click events over time, displaying them in a line chart to observe trends or patterns.",
      "query": "\nSELECT\n    dt,\n    count(*) AS c,\n    bar(c, 0, 100000)\nFROM random.click_events\nGROUP BY dt\nORDER BY dt ASC",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "randnormal-distribution-of-clicks"
    },
    {
      "id": "GEFQJZTCMQPSB8ZMDCHJWK",
      "name": "Show create table hits",
      "group": "metrica",
      "comment": "Requests the SQL command used to create the 'hits' table in the 'metrica' database.",
      "query": "SHOW CREATE TABLE metrica.hits",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "show-create-table-hits"
    },
    {
      "id": "537DNRRQ1XJBOTNA9UXWPA",
      "name": "Total amount of hits for each day",
      "group": "wiki",
      "comment": "Aggregates total daily hits from the 'wiki.wikistat_small' dataset and displays them in a line chart format, ordered sequentially by date.",
      "query": "SELECT\n    sum(hits) AS h,\n    toDate(time) AS d\nFROM wiki.wikistat_small\nGROUP BY d\nORDER BY d\nLIMIT 5",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "total-amount-of-hits-for-each-day"
    },
    {
      "id": "9GJNUGQSWRRPZRPMFVD2DU",
      "name": "Total amount of hits for each hour",
      "group": "wiki",
      "comment": "Calculates and orders the total number of hits per hour for a specific date in ascending order, limited to the first five hours.",
      "query": "SELECT\n    sum(hits) AS v,\n    toStartOfHour(time) AS h\nFROM wiki.wikistat_small\nWHERE date(time) = '2015-05-01'\nGROUP BY h\nORDER BY h ASC\nLIMIT 5",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "total-amount-of-hits-for-each-hour"
    },
    {
      "id": "7HPC3EXF1K3CCFEZGJ36XT",
      "name": "Total amount of hits by 4-hour intervals",
      "group": "wiki",
      "comment": "Calculates the total number of hits every 4 hours on May 1, 2015, from the wiki data, sorting the results in ascending order and limiting the output to the first six intervals.",
      "query": "SELECT\n    sum(hits) AS v,\n    toStartOfInterval(time, toIntervalHour(4)) AS h\nFROM wiki.wikistat_small\nWHERE date(time) = '2015-05-01'\nGROUP BY h\nORDER BY h ASC\nLIMIT 6",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "total-amount-of-hits-by-4-hour-intervals"
    },
    {
      "id": "AWOMAV5AQUHFUHVXZ5UGDR",
      "name": "Total amount of hits for each hour with missing intervals",
      "group": "wiki",
      "comment": "Calculates the sum of hits per hour on June 12, 2015, for the Italian mobile subproject and displays the data in a line chart, despite missing intervals.",
      "query": "SELECT\n    toStartOfHour(time) AS h,\n    sum(hits)\nFROM wiki.wikistat_small\nWHERE (project = 'it') AND (subproject = 'm') AND (date(time) = '2015-06-12')\nGROUP BY h\nORDER BY h ASC\n",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "total-amount-of-hits-for-each-hour-with-missing-intervals"
    },
    {
      "id": "PUNOYTV9U37JUHQHDBK1RB",
      "name": "Total amount of hits for each hour with filled intervals",
      "group": "wiki",
      "comment": "Calculates the total amount of hits per hour for a specific date, ensuring hourly intervals are filled even if no data is available.",
      "query": "SELECT\n    toStartOfHour(time) AS h,\n    sum(hits)\nFROM wiki.wikistat_small\nWHERE (project = 'it') AND (subproject = 'm') AND (date(time) = '2015-06-12')\nGROUP BY h\nORDER BY h ASC WITH FILL STEP toIntervalHour(1)",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "total-amount-of-hits-for-each-hour-with-filled-intervals"
    },
    {
      "id": "NT9TMF54T6ZD6EIJOQSWSQ",
      "name": "Rolling time window",
      "group": "wiki",
      "comment": "Calculates the daily sum of hits from the specified starting date, grouping results by day and displaying the first five days in ascending order.",
      "query": "SELECT\n    sum(hits),\n    dateDiff('day', toDateTime('2015-05-01 18:00:00'), toDateTime(time)) AS d\nFROM wiki.wikistat_small\nGROUP BY d\nORDER BY d ASC\nLIMIT 5",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "rolling-time-window"
    },
    {
      "id": "7FM7GEM6MGK3I4Y916BRKK",
      "name": "Visualize the most and least popular hours in terms of page views",
      "group": "wiki",
      "comment": "Visualizes hourly page view popularity using a bar chart within a line graph to highlight peak and low traffic times.",
      "query": "SELECT\n    toHour(time) AS h,\n    sum(hits) AS t,\n    bar(t, 0, max(t) OVER ())\nFROM wiki.wikistat_small\nGROUP BY h\nORDER BY h ASC",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "visualize-the-most-and-least-popular-hours-in-terms-of-page-views"
    },
    {
      "id": "JAEWDDPZHMJOVSZEDD3RVB",
      "name": "Distribution of a number of pages based on their total hits",
      "group": "wiki",
      "comment": "Analyzes and visualizes the distribution of page hits above 10,000 by creating a histogram using data from June 15, 2015, and represents it with a line chart.",
      "query": "WITH histogram(10)(hits) AS h\nSELECT\n    round(arrayJoin(h).1) AS l,\n    round(arrayJoin(h).2) AS u,\n    arrayJoin(h).3 AS w,\n    bar(w, 0, max(w) OVER (), 20) AS b\nFROM\n(\n    SELECT\n        path,\n        sum(hits) AS hits\n    FROM wiki.wikistat_small\n    WHERE date(time) = '2015-06-15'\n    GROUP BY path\n    HAVING hits > 10000.\n)",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "distribution-of-a-number-of-pages-based-on-their-total-hits"
    },
    {
      "id": "5GDTHIPSNWHKXN1KUSQSMJ",
      "name": "Daily hits for a given page",
      "group": "wiki",
      "comment": "Analyzes the daily trend of hits for a specific wiki page by comparing each day's hits to the overall pattern.",
      "query": "SELECT\n    toDate(time) AS d,\n    sum(hits) AS h,\n    lagInFrame(h) OVER (ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS p,\n    h - p AS trend\nFROM wiki.wikistat_small\nWHERE path = 'Bob'\nGROUP BY d\nORDER BY d ASC\nLIMIT 15",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "daily-hits-for-a-given-page"
    },
    {
      "id": "CRHBCIOY1ZKMX6ZK7DXG7Z",
      "name": "Page cumulative growth",
      "group": "wiki",
      "comment": "Analyzes daily cumulative growth in page hits for a specific wiki article over time, with a visual representation using a bar chart.",
      "query": "SELECT\n    toDate(time) AS d,\n    sum(hits) AS h,\n    sum(h) OVER (ROWS BETWEEN UNBOUNDED PRECEDING AND 0 FOLLOWING) AS c,\n    bar(c, 0, 3200000, 25) AS b\nFROM wiki.wikistat_small\nWHERE path = 'Bob'\nGROUP BY d\nORDER BY d ASC\nLIMIT 15",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "page-cumulative-growth"
    },
    {
      "id": "XQVSQGEZEZUZUAQURVO8VZ",
      "name": "Hit rate per second for a given date grouped by hours",
      "group": "wiki",
      "comment": "Calculates hit rate per second for \"Bob\" page views, grouped by hour with a visual bar representation of the rate.",
      "query": "SELECT\n    toStartOfHour(time) AS t,\n    sum(hits) AS h,\n    round(h / (60 * 60), 2) AS rate,\n    bar(rate * 10, 0, max(rate * 10) OVER (), 25) AS b\nFROM wiki.wikistat_small\nWHERE path = 'Bob'\nGROUP BY t\nORDER BY t ASC\nLIMIT 23",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "hit-rate-per-second-for-a-given-date-grouped-by-hours"
    },
    {
      "id": "G7YRTCRZ7RJPMTZCRFCPDB",
      "name": "Number of projects",
      "group": "wiki",
      "comment": "Counts the unique projects and subprojects in the 'wiki.wikistat_small' database for analysis.",
      "query": "SELECT\n    uniq(project),\n    uniq(subproject)\nFROM wiki.wikistat_small",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "number-of-projects"
    },
    {
      "id": "DE5AJMQ3DLWAPSOJP1PI9C",
      "name": "Select max hits",
      "group": "wiki",
      "comment": "Find the maximum number of hits recorded from the wiki.wikistat_small dataset.",
      "query": "SELECT max(hits)\nFROM wiki.wikistat_small",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "select-max-hits"
    },
    {
      "id": "HPGEZK7USHGDUGTJTHJAYV",
      "name": "Sum hits per project",
      "group": "wiki",
      "comment": "Sums up the total hits for each project and lists the top 10 projects by hit count in descending order.",
      "query": "SELECT\n    project,\n    sum(hits) AS h\nFROM wiki.wikistat_small\nGROUP BY project\nORDER BY h DESC\nLIMIT 10\n",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "sum-hits-per-project"
    },
    {
      "id": "GFCCTYVJ3YVGMTOFXDQZJT",
      "name": "Sum hits per project - Optimized",
      "group": "wiki",
      "comment": "Summarizes total hits per project from a dataset, displaying the top ten results in descending order by hit count.",
      "query": "SELECT\n    project,\n    sum(hits) AS h\nFROM wiki.optimized_wikistat_small\nGROUP BY project\nORDER BY h DESC\nLIMIT 10",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "sum-hits-per-project-optimized"
    },
    {
      "id": "2Q8G2SWXOOSHHVVBQB8ND2",
      "name": "Sum hits per sub-project",
      "group": "wiki",
      "comment": "Aggregates and ranks the total number of hits for each sub-project within the Italian wiki projects based on descending order.",
      "query": "SELECT\n    subproject,\n    sum(hits) AS h\nFROM wiki.wikistat_small\nWHERE project = 'it'\nGROUP BY subproject\nORDER BY h DESC\nLIMIT 10",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "sum-hits-per-sub-project"
    },
    {
      "id": "1G21SEVPJZM1XFRTPR6MVR",
      "name": "Sum hits per sub-project - Optimized",
      "group": "wiki",
      "comment": "Calculates and ranks the sum of hits for each sub-project within a specified project from the optimized_wikistat_small table, presenting the top 10 in descending order.",
      "query": "SELECT\n    subproject,\n    sum(hits) AS h\nFROM wiki.optimized_wikistat_small\nWHERE project = 'it'\nGROUP BY subproject\nORDER BY h DESC\nLIMIT 10",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "sum-hits-per-sub-project-optimized"
    },
    {
      "id": "4TB6QY6HMELIW5PUWRNG4M",
      "name": "Sum hits for a specific project and sub-project",
      "group": "wiki",
      "comment": "Calculates the monthly sum of hits for a specified project and sub-project, displaying the results in descending order.",
      "query": "SELECT\n    toStartOfMonth(time) AS m,\n    sum(hits) AS h\nFROM wiki.wikistat_small\nWHERE (project = 'it') AND (subproject = 'zero')\nGROUP BY m\nORDER BY m DESC\nLIMIT 10",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "sum-hits-for-a-specific-project-and-sub-project"
    },
    {
      "id": "9EBNNA5MNGKSDJMNLGMCHB",
      "name": "Sum hits for a specific project and sub-project - Optimized",
      "group": "wiki",
      "comment": "Calculates the monthly sum of hits for the 'it' project and 'zero' subproject, displaying the data in descending order for the last 10 months.",
      "query": "SELECT\n    toStartOfMonth(time) AS m,\n    sum(hits) AS h\nFROM wiki.optimized_wikistat_small\nWHERE (project = 'it') AND (subproject = 'zero')\nGROUP BY m\nORDER BY m DESC\nLIMIT 10",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "sum-hits-for-a-specific-project-and-sub-project-optimized"
    },
    {
      "id": "WW6JBMONCU5VKCKQXSYTFM",
      "name": "Sum hits for a specific project and sub-project ordered by path",
      "group": "wiki",
      "comment": "Calculate and display the top 10 paths with the highest sum of hits for the specified project and sub-project.",
      "query": "SELECT\n    path,\n    sum(hits) AS h\nFROM wiki.wikistat_small\nWHERE (project = 'it') AND (subproject = 'zero')\nGROUP BY path\nORDER BY h DESC\nLIMIT 10",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "sum-hits-for-a-specific-project-and-sub-project-ordered-by-path"
    },
    {
      "id": "3EYNNE9LVWLHSAH2MD4N3Y",
      "name": "Sum hits for a specific project and sub-project ordered by path - Optimized",
      "group": "wiki",
      "comment": "Sums and ranks the hits for paths within the specified project and subproject in descending order of hits, limiting results to the top 10.",
      "query": "SELECT\n    path,\n    sum(hits) AS h\nFROM wiki.optimized_wikistat_small\nWHERE (project = 'it') AND (subproject = 'zero')\nGROUP BY path\nORDER BY h DESC\nLIMIT 10",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "sum-hits-for-a-specific-project-and-sub-project-ordered-by-path-optimized"
    },
    {
      "id": "PSZHGXKFFDGDRADCAMCF1Q",
      "name": "Top path using materialized views",
      "group": "wiki",
      "comment": "Displays the top 10 most visited wiki paths in May 2015 using hit count data sorted in descending order.",
      "query": "SELECT\n    path,\n    hits\nFROM wiki.wikistat_top\nWHERE month = '2015-05-01'\nORDER BY hits DESC\nLIMIT 10",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "top-path-using-materialized-views"
    },
    {
      "id": "5RZTNTHWWQKVMZHBTXSRG2",
      "name": "Select view search_clickhouse_stackoverflow",
      "group": "stackoverflow",
      "comment": "Fetches all records from the search_clickhouse_stackoverflow view in the stackoverflow dataset for analysis.",
      "query": "SELECT *\nFROM stackoverflow.search_clickhouse_stackoverflow",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "select-view-search_clickhouse_stackoverflow"
    },
    {
      "id": "OVVFRFJVZXTWGQDB2CQDU7",
      "name": "Search post mentioning ClickHouse MergeTree",
      "group": "stackoverflow",
      "comment": "Retrieves the top Stack Overflow post mentioning 'ClickHouse MergeTree' based on score.",
      "query": "SELECT *\nFROM stackoverflow.search_stackoverflow(text = 'ClickHouse MergeTree')\nORDER BY Score DESC\nLIMIT 1",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "search-post-mentioning-clickhouse-mergetree"
    },
    {
      "id": "AACTS8ZBT3G7SSGN8ZJBJY",
      "name": "Show tables",
      "group": "imdb",
      "comment": "Lists all tables available in the IMDb database.",
      "query": "SHOW TABLES FROM imdb;",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "show-tables"
    },
    {
      "id": "SXBYSHJHMVZQTTA8NJFXIJ",
      "name": "Fnds the genre for each movie",
      "group": "imdb",
      "comment": "Lists the genre for each movie by joining movies and genres tables, sorted by year, movie name, and genre.",
      "query": "SELECT\n    m.name AS name,\n    g.genre AS genre\nFROM imdb.movies AS m\nINNER JOIN imdb.genres AS g ON m.id = g.movie_id\nORDER BY\n    m.year DESC,\n    m.name ASC,\n    g.genre ASC\nLIMIT 10;\n",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "fnds-the-genre-for-each-movie"
    },
    {
      "id": "ULZ1D3RO8UIJ7OGNEWFPJJ",
      "name": "Finds all movies with no genre",
      "group": "imdb",
      "comment": "Lists the top 10 movies without any associated genre, sorted by year in descending order and name in ascending order.",
      "query": "SELECT m.name\nFROM imdb.movies AS m\nLEFT JOIN imdb.genres AS g ON m.id = g.movie_id\nWHERE g.movie_id = 0\nORDER BY\n    m.year DESC,\n    m.name ASC\nLIMIT 10;",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "finds-all-movies-with-no-genre"
    },
    {
      "id": "IGYXD5K3FHANTAEFSXFRNZ",
      "name": "Cross joins movie and genre tables",
      "group": "imdb",
      "comment": "Displays a list of movie names and genres by cross joining movies with genres, limited to 10 results.",
      "query": "SELECT\n    m.name,\n    m.id,\n    g.movie_id,\n    g.genre\nFROM imdb.movies AS m\nCROSS JOIN imdb.genres AS g\nLIMIT 10;\n",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "cross-joins-movie-and-genre-tables"
    },
    {
      "id": "ITEJZPXTD1CNGRAZHVBJUY",
      "name": "Finds the genre for each movie using Cross joins",
      "group": "imdb",
      "comment": "Determines the genre for each movie by executing a cross join between movie and genre tables, filtering on matching movie IDs, and sorting by year and name.",
      "query": "SELECT\n    m.name,\n    g.genre\nFROM imdb.movies AS m\nCROSS JOIN imdb.genres AS g\nWHERE m.id = g.movie_id\nORDER BY\n    m.year DESC,\n    m.name ASC\nLIMIT 10;\n\n",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "finds-the-genre-for-each-movie-using-cross-joins"
    },
    {
      "id": "P8JVPYHHCSWLTAY1JCSZXQ",
      "name": "Cross joins explain",
      "group": "imdb",
      "comment": "Explains how to execute a cross join between movies and genres from the IMDb database, ordered by descending year and ascending names and genres, with a limit on the number of results.",
      "query": "EXPLAIN SYNTAX\nSELECT\n    m.name AS name,\n    g.genre AS genre\nFROM imdb.movies AS m\nCROSS JOIN imdb.genres AS g\nWHERE m.id = g.movie_id\nORDER BY\n    m.year DESC,\n    m.name ASC,\n    g.genre ASC\nLIMIT 10;\n",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "cross-joins-explain"
    },
    {
      "id": "2T1SYGUTWFFZBW7EEZQB3F",
      "name": "Finds all persons that performed in a movie in 2023",
      "group": "imdb",
      "comment": "Identifies actors who participated in movies released in 2023.",
      "query": "SELECT\n    a.first_name,\n    a.last_name\nFROM imdb.actors AS a\nLEFT SEMI JOIN imdb.roles AS r ON a.id = r.actor_id\nWHERE toYear(created_at) = '2023'\nORDER BY id ASC\nLIMIT 10;\n",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "finds-all-persons-that-performed-in-a-movie-in-2023"
    },
    {
      "id": "3R1AT8GC5S4JHPZSGKC6K4",
      "name": "Find movie with no genre using anti join",
      "group": "imdb",
      "comment": "List the top 10 movies without an associated genre, ordered by release year in descending order and name in ascending order.",
      "query": "SELECT m.name\nFROM imdb.movies AS m\nLEFT ANTI JOIN imdb.genres AS g ON m.id = g.movie_id\nORDER BY\n    year DESC,\n    name ASC\nLIMIT 10;\n",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "find-movie-with-no-genre-using-anti-join"
    },
    {
      "id": "TJQUE4JPEWUPWVV8RYWBTA",
      "name": "LEFT ANY JOIN example",
      "group": "join",
      "comment": "Illustrates the use of LEFT ANY JOIN to combine two tables with duplicate values based on matching columns, ensuring each left table row matches with only one right table row.",
      "query": "WITH\n    left_table AS (SELECT * FROM VALUES('c UInt32', 1, 2, 3)),\n    right_table AS (SELECT * FROM VALUES('c UInt32', 2, 2, 3, 3, 4))\nSELECT\n    l.c AS l_c,\n    r.c AS r_c\nFROM left_table AS l\nLEFT ANY JOIN right_table AS r ON l.c = r.c;\n",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "left-any-join-example"
    },
    {
      "id": "OYVCDZYVGI7LFDAAZJ8DQG",
      "name": "RIGHT ANY JOIN example",
      "group": "join",
      "comment": "Demonstrates using a RIGHT ANY JOIN to combine data from two tables by matching rows on a common column.",
      "query": "WITH\n    left_table AS (SELECT * FROM VALUES('c UInt32', 1, 2, 3)),\n    right_table AS (SELECT * FROM VALUES('c UInt32', 2, 2, 3, 3, 4))\nSELECT\n    l.c AS l_c,\n    r.c AS r_c\nFROM left_table AS l\nRIGHT ANY JOIN right_table AS r ON l.c = r.c;\n",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "right-any-join-example"
    },
    {
      "id": "GJMMKQZX1UTRFW6MZSAYCH",
      "name": "INNER ANY JOIN example",
      "group": "join",
      "comment": "Demonstrates an INNER ANY JOIN operation between two tables based on a common column.",
      "query": "WITH\n    left_table AS (SELECT * FROM VALUES('c UInt32', 1, 2, 3)),\n    right_table AS (SELECT * FROM VALUES('c UInt32', 2, 2, 3, 3, 4))\nSELECT\n    l.c AS l_c,\n    r.c AS r_c\nFROM left_table AS l\nINNER ANY JOIN right_table AS r ON l.c = r.c;\n",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "inner-any-join-example"
    },
    {
      "id": "9WOMEAY1CFZUVQN6DZXRY7",
      "name": "Show formats",
      "group": "system",
      "comment": "Displays all available data formats in the system.",
      "query": "SELECT *\nFROM system.formats",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "show-formats"
    },
    {
      "id": "MNVFKT7APRSXKVDVQYM5JB",
      "name": "Show table engines",
      "group": "system",
      "comment": "Lists all available table engines in the system with details.",
      "query": "SELECT *\nFROM system.table_engines",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "show-table-engines"
    },
    {
      "id": "IQPQW45RJGWDCV66FT4D6T",
      "name": "Show functions",
      "group": "system",
      "comment": "Lists all system-level functions from the database to display their details and origins.",
      "query": "SELECT *\nFROM system.functions\nWHERE origin = 'System'",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "show-functions"
    },
    {
      "id": "9RNS3C9SDFSCDV2AOXW8A6",
      "name": "Show function per category",
      "group": "system",
      "comment": "Filters and lists table functions and engines excluding specific patterns and names, ordered alphabetically.",
      "query": "WITH both AS (\n        SELECT name, 'Table function' as category\n        FROM system.table_functions \n    UNION ALL\n        SELECT name, 'Table engine' as category\n        FROM system.table_engines\n)\nSELECT * \nFROM both\nWHERE \n    NOT name ilike '%mergeTree%' AND\n    NOT name ilike '%view%' AND\n    NOT name ilike '%values%' AND\n    NOT name ilike '%zeros%' AND\n    NOT name ilike '%cosn%' AND\n    NOT name ilike '%cosn%' AND\n    NOT name ilike '%buffer%' AND\n    NOT name ilike '%replica%' AND\n    NOT name ilike '%distributed%' AND\n    NOT name ilike '%json%' AND\n    NOT name ilike '%random%' AND\n    NOT name ilike '%merge%'AND\n    NOT name ilike '%null%'AND\n    NOT name ilike '%numbers%'AND\n    NOT name ilike '%oss%'AND\n    NOT name IN ['cluster', 'format', 'input', 'Join', 'KeeperMap', 'Log', 'Memory', 'Set', 'StripeLog', 'TinyLog']    \nORDER BY lower(name)",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "show-function-per-category"
    },
    {
      "id": "4MGH76GE6QN6WA6H8TCYKR",
      "name": "Top 10 busiest stations in 2018",
      "group": "mta",
      "comment": "Identifies the top 10 subway stations with the highest total entries in 2018 using MTA transit data.",
      "query": "SELECT station, sum(entries_change) as total_entries, formatReadableQuantity(total_entries) as total_entries_read\nFROM mta.subway_transits_2014_2022_v2\nWHERE toYear(date_time) = '2018'\nGROUP BY station \nORDER BY sum(entries_change) DESC\nLIMIT 10",
      "chart": {
        "type": "bar",
        "config": {
          "xaxis": "station",
          "yaxis": "total_entries",
          "series": "station",
          "stack": true
        }
      },
      "format": false,
      "params": [],
      "slug": "top-10-busiest-stations-in-2018"
    },
    {
      "id": "KADCUSBZG3UWVV2N4QUJXW",
      "name": "Change in traffic over the years for busiest station",
      "group": "mta",
      "comment": "Analyzes the yearly change in subway entries for the top 10 busiest stations over an eight-year period, visualized in a line chart format.",
      "query": "SELECT\n  station,\n  toYear(date_time) as year,\n  sum(entries_change) as total_entries\nFROM\n  mta.subway_transits_2014_2022_v2\nWHERE\n  station IN (\n    SELECT station FROM mta.subway_transits_2014_2022_v2 GROUP BY station ORDER BY sum(entries_change) DESC LIMIT 10\n  )\nGROUP BY\n  year,\n  station\nORDER BY\n  year\n",
      "chart": {
        "type": "line",
        "config": {
          "xaxis": "year",
          "yaxis": "total_entries",
          "series": "station",
          "stack": false
        }
      },
      "format": false,
      "params": [],
      "slug": "change-in-traffic-over-the-years-for-busiest-station"
    },
    {
      "id": "STHDUVXOFZFGF2JCHJGB5Y",
      "name": "Traffic comparison weekdays versus weekend",
      "group": "mta",
      "comment": "Compares total transit ridership between weekdays and weekend, grouping data by the start of each week.",
      "query": "SELECT\n    toStartOfWeek(transit_timestamp) as week,\n    'weekday' as period,\n    sum(ridership) AS total\nFROM mta.transit_data WHERE toDayOfWeek(transit_timestamp) <= 5\nGROUP BY week ORDER BY week ASC\nUNION ALL\nSELECT\n    toStartOfWeek(transit_timestamp) as week,\n    'weekend' as period,\n    sum(ridership) AS total\nFROM mta.transit_data WHERE toDayOfWeek(transit_timestamp) > 5\nGROUP BY week ORDER BY week ASC",
      "chart": {
        "type": "line",
        "config": {
          "xaxis": "week",
          "yaxis": "total",
          "series": "period"
        }
      },
      "format": false,
      "params": [],
      "slug": "traffic-comparison-weekdays-versus-weekend"
    },
    {
      "id": "HPN5AHXEHK1NM2NB9S3AV2",
      "name": "Average hourly traffic by hour of the day",
      "group": "mta",
      "comment": "Calculates the average hourly entries by station complex on weekdays, displaying the top three busiest stations per hour in a stacked bar chart.",
      "query": "SELECT\n    station_complex, toHour(hour_of_day) as hour, avg(total_entries):: UInt64 AS avg_entries\nFROM\n(\n    SELECT\n        toStartOfHour(transit_timestamp) AS hour_of_day,\n        station_complex,\n        sum(ridership) AS total_entries\n    FROM mta.transit_data WHERE toDayOfWeek(transit_timestamp) <= 5\n    GROUP BY\n        station_complex,\n        hour_of_day\n)\nGROUP BY hour, station_complex ORDER BY hour ASC, avg_entries DESC LIMIT 3 BY hour",
      "chart": {
        "type": "bar",
        "config": {
          "xaxis": "hour",
          "yaxis": "avg_entries",
          "series": "station_complex",
          "stack": true
        }
      },
      "format": false,
      "params": [],
      "slug": "average-hourly-traffic-by-hour-of-the-day"
    },
    {
      "id": "7VVBISZSVYWTAWR8LUDSTE",
      "name": "Sample tweets from a specific user",
      "group": "twitter",
      "comment": "Displays a chronological list of tweets by 'elonmusk' with their creation dates.",
      "query": "SELECT created_at, text FROM twitter.twitter WHERE tupleElement(user, 'screen_name') = 'elonmusk' ORDER BY created_at",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "sample-tweets-from-a-specific-user"
    },
    {
      "id": "ASSNFTKPUROV9QRCS5FATTT",
      "name": "Temperature by country and year",
      "group": "noaa",
      "comment": "99th percentile of avg temperature by country and year",
      "query": "SELECT\n  toStartOfYear(`date`) AS `year`,\n  quantileExact(0.99)(`tempAvg` / 10) AS `99th_avg_temp`,\n  dictGet(`country`.`country_iso_codes`, 'name', code) AS country\nFROM\n  `noaa`.`noaa_v2`\nWHERE\n  date > '1990-01-01'\n  AND code IN ('FR', 'UK', 'IN', 'NZ', 'SP', 'US')\nGROUP BY\n  year,\n  substring(station_id, 1, 2) AS code\nHAVING\n  99th_avg_temp > 0\nORDER BY\n  country,\n  year ASC\nLIMIT\n  100000;",
      "chart": {
        "type": "line",
        "config": {
          "xaxis": "year",
          "yaxis": "99th_avg_temp",
          "series": "country",
          "title": "Temperature by country and year"
        }
      },
      "format": false,
      "params": [],
      "slug": "temperature-by-country-and-year"
    },
    {
      "id": "XNHZCZZOKEPJCQYJH9IJH9",
      "name": "Recent releases",
      "group": "pypi",
      "comment": "Tracks newly released versions of specified Python packages from the PyPI repository over the last six months, grouped and displayed by release month.",
      "query": "--Recently released Python packages\nWITH (\n  SELECT\n    max(upload_time) AS max_date\n  FROM\n    pypi.projects\n) AS max_date\nSELECT\n  release_month as x,\n  name as y,\n  uniqExact(version) AS z\nFROM\n  pypi.projects\nWHERE\n  (name IN { packages: Array(String) })\n  AND (\n    toStartOfMonth(upload_time) > toStartOfMonth(max_date - toIntervalMonth(6))\n  )\nGROUP BY\n  name,\n  toMonth(upload_time) AS month,\n  formatDateTime(upload_time, '%b') AS release_month\nORDER BY\n  month ASC\nLIMIT\n  30",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": {
        "packages": "['boto3','urllib3','botocore','requests','setuptools']"
      },
      "slug": "recent-releases"
    },
    {
      "id": "UEZEBJHYKTWDYXQZFMUT39",
      "name": "Top projects",
      "group": "pypi",
      "comment": "Identifies the top 5 most downloaded projects on PyPI by summing up the download counts and sorting them in descending order.",
      "query": "--Most downloaded projects\nSELECT\n  project,\n  sum(count) AS c\nFROM\n  pypi.pypi_downloads\nGROUP BY\n  project\nORDER BY\n  c DESC\nLIMIT\n  5",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "top-projects"
    },
    {
      "id": "TZE9ISWDW4UMB3BYVVZTSA",
      "name": "Total downloads and projects",
      "group": "pypi",
      "comment": "Calculates the total number of downloads and unique projects from the PyPI downloads dataset.",
      "query": "SELECT formatReadableQuantity(sum(count)) AS total, uniqExact(project) as projects FROM pypi.pypi_downloads",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "total-downloads-and-projects"
    },
    {
      "id": "51KVUJ5FGJUQV9XU13JKL3",
      "name": "When do people use BlueSky?",
      "group": "bluesky",
      "comment": "Analyzes the frequency of posts, reposts, and likes on BlueSky across different hours of the day.",
      "query": "SELECT event, hour_of_day, sum(count) as count\nFROM bluesky.events_per_hour_of_day\nWHERE event in ['post', 'repost', 'like']\nGROUP BY event, hour_of_day\nORDER BY hour_of_day;",
      "chart": {
        "type": "bar",
        "config": {
          "title": "Number of events per hour of day",
          "xaxis": "hour_of_day",
          "yaxis": "count",
          "series": "event",
          "stack": false
        }
      },
      "format": false,
      "params": [],
      "slug": "when-do-people-use-bluesky"
    },
    {
      "id": "SH7GXXK4BYPHELXM5AHWDJ",
      "name": "Most reposted users",
      "group": "bluesky",
      "comment": "Identifies the top 10 users with the highest number of total reposts by summing reposts and sorting in descending order.",
      "query": "SELECT\n    handle,\n    sum(reposts) AS reposts\nFROM bluesky.reposts_per_user AS rpu\nINNER JOIN bluesky.handle_per_user AS hpu ON rpu.did = hpu.did\nGROUP BY ALL\nORDER BY reposts DESC\nLIMIT 10",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "most-reposted-users"
    },
    {
      "id": "H2CYZJDRCXCFYLPXMJVRCR",
      "name": "Most liked users",
      "group": "bluesky",
      "comment": "Identifies and ranks the top ten users based on the total number of likes received.",
      "query": "SELECT\n    handle,\n    sum(likes) AS likes\nFROM bluesky.likes_per_user AS lpu\nINNER JOIN bluesky.handle_per_user AS hpu ON lpu.did = hpu.did\nGROUP BY ALL\nORDER BY likes DESC\nLIMIT 10",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "most-liked-users"
    },
    {
      "id": "ATT83TXCQE8DUDCM84GB6E",
      "name": "Most used languages",
      "group": "bluesky",
      "comment": "Ranks the top 10 languages based on the total number of posts from the bluesky.posts_per_language database.",
      "query": "SELECT\n    language,\n    sum(posts) as posts\nFROM bluesky.posts_per_language\nGROUP BY language\nORDER BY posts DESC\nLIMIT 10;",
      "chart": {
        "type": "horizontal bar",
        "config": {
          "xaxis": "language",
          "yaxis": "posts",
          "title": "Most used languages"
        }
      },
      "format": false,
      "params": [],
      "slug": "most-used-languages"
    },
    {
      "id": "BP4SSVSKXB4EJCMFHU8B6D",
      "name": "Most reposted posts",
      "group": "bluesky",
      "comment": "Identifies the top 10 most reposted posts by summing reposts per post and joining with their corresponding text.",
      "query": "WITH top_liked_cids AS\n(\n    SELECT\n        cid,\n        sum(reposts) AS reposts\n    FROM bluesky.reposts_per_post\n    GROUP BY cid\n    ORDER BY reposts DESC\n    LIMIT 10\n)\nSELECT\n    t1.reposts,\n    t2.text\nFROM top_liked_cids AS t1\nLEFT JOIN\n(\n    -- by exploiting its sorting key (cid),\n    -- we manually pre-filter the right join partner\n    SELECT *\n    FROM bluesky.cid_to_text\n    WHERE cid IN (SELECT cid FROM top_liked_cids)\n) AS t2 ON t1.cid = t2.cid;",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "most-reposted-posts"
    },
    {
      "id": "8YAFPZQXXCGD75842UKE2W",
      "name": "Most liked posts",
      "group": "bluesky",
      "comment": "Identifies the top 10 posts with the highest number of likes and retrieves their text content.",
      "query": "WITH top_liked_cids AS\n(\n    SELECT\n        cid,\n        SUM(likes) AS likes\n    FROM bluesky.likes_per_post\n    GROUP BY cid\n    ORDER BY likes DESC\n    LIMIT 10\n)\nSELECT\n    t1.likes,\n    t2.text\nFROM top_liked_cids AS t1\nLEFT JOIN\n(\n    -- by exploiting its sorting key (cid),\n    -- we manually pre-filter the right join partner\n    SELECT *\n    FROM bluesky.cid_to_text\n    WHERE cid IN (SELECT cid FROM top_liked_cids)\n) AS t2 ON t1.cid = t2.cid;",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "most-liked-posts"
    },
    {
      "id": "7ICWGHB7HIIEWCMYFAWIAE",
      "name": "Most liked posts about ClickHouse",
      "group": "bluesky",
      "comment": "Retrieve the top three most liked posts about ClickHouse by summing likes per post and joining with post texts.",
      "query": "WITH top_liked_cids AS\n(\n    SELECT\n        cid,\n        SUM(likes) AS likes\n    FROM bluesky.likes_per_post_about_clickhouse\n    GROUP BY cid\n    ORDER BY likes DESC\n    LIMIT 3\n)\nSELECT\n    t1.likes,\n    t2.text\nFROM top_liked_cids AS t1\nLEFT JOIN\n(\n    -- by exploiting its sorting key (cid),\n    -- we manually pre-filter the right join partner\n    SELECT *\n    FROM bluesky.cid_to_text\n    WHERE cid IN (SELECT cid FROM top_liked_cids)\n) AS t2 ON t1.cid = t2.cid;",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "most-liked-posts-about-clickhouse"
    },
    {
      "id": "RJR6SMBYEKJSSWWUXFHP1U",
      "name": "Top event types by count",
      "group": "bluesky",
      "comment": "Counts and orders top event types by number of posts from the bluesky dataset, displaying results in a horizontal bar chart.",
      "query": "SELECT collection,\n       sum(posts) AS posts\nFROM bluesky.top_post_types\nGROUP BY collection\nORDER BY posts DESC;",
      "chart": {
        "type": "horizontal bar",
        "config": {
          "xaxis": "collection",
          "yaxis": "posts",
          "title": "Top event types by count"
        }
      },
      "format": false,
      "params": [],
      "slug": "top-event-types-by-count"
    },
    {
      "id": "5C6SW7OHKEVFLRDMED2WNB",
      "name": "Top event types by unique users",
      "group": "bluesky",
      "comment": "Determines and ranks the top event types by the number of unique users associated with each type.",
      "query": "SELECT collection,\n uniqMerge(users) AS users\nFROM bluesky.top_post_types\nGROUP BY collection\nORDER BY users DESC;",
      "chart": {
        "type": "horizontal bar",
        "config": {
          "xaxis": "collection",
          "yaxis": "users",
          "title": "Top event types by unique users"
        }
      },
      "format": false,
      "params": [],
      "slug": "top-event-types-by-unique-users"
    },
    {
      "id": "9WMMTPMMP7TAIWO5ZGWZZE",
      "name": "Top event types",
      "group": "bluesky",
      "comment": "Analyzes and ranks the most popular event types by the number of posts, also showing unique user engagement for each type.",
      "query": "SELECT collection,\n  sum(posts) AS posts,\n  uniqMerge(users) AS users\nFROM bluesky.top_post_types\nGROUP BY collection\nORDER BY posts DESC;",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "top-event-types"
    },
    {
      "id": "P2VKEOGYQVHFPA8F5IFZ2P",
      "name": "Total events",
      "group": "bluesky",
      "comment": "Counts the total number of events from the bluesky database.",
      "query": "\nSELECT count() FROM bluesky.bluesky;",
      "chart": {
        "type": "line"
      },
      "format": false,
      "params": [],
      "slug": "total-events"
    }
  ]
}